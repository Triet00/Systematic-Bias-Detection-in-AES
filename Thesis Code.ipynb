{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00848838",
   "metadata": {},
   "source": [
    "## 1. PREPROCESSING AND STANDARDIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85032056",
   "metadata": {},
   "source": [
    "### 1.1 Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def1083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize essays in a CSV file\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download tokenizer\n",
    "try:\n",
    "    nltk.download('punkt_tab')\n",
    "except:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "def standardize_essay(text):\n",
    "    \"\"\"\n",
    "    Standardizes a single essay:\n",
    "    - Splits paragraphs on double newlines\n",
    "    - Splits sentences within paragraphs\n",
    "    - Ensures one sentence per line\n",
    "    - Ensures paragraphs are separated by double newlines\n",
    "    \"\"\"\n",
    "    standardized_paragraphs = []\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    for p in paragraphs:\n",
    "        sentences = sent_tokenize(p)\n",
    "        standardized_paragraphs.append('\\n'.join(sentences))\n",
    "    \n",
    "    standardized_text = '\\n\\n'.join(standardized_paragraphs)\n",
    "    return standardized_text\n",
    "\n",
    "def extract_overall_score(evaluation_text):\n",
    "    \"\"\"\n",
    "    Extracts the overall score (last number) from the evaluation column.\n",
    "    Looks for numbers that could be scores (integers or decimals).\n",
    "    \"\"\"\n",
    "    if pd.isna(evaluation_text):\n",
    "        return None\n",
    "    \n",
    "    # Find all numbers (integers and decimals) in the text\n",
    "    numbers = re.findall(r'\\b\\d+(?:\\.\\d+)?\\b', str(evaluation_text))\n",
    "    \n",
    "    if numbers:\n",
    "        # Return the last number found (overall score)\n",
    "        return float(numbers[-1])\n",
    "    \n",
    "    return None\n",
    "\n",
    "def standardize_band(band_value):\n",
    "    \"\"\"\n",
    "    Standardizes band score format:\n",
    "    - Removes leading/trailing whitespace\n",
    "    - Ensures consistent format (e.g., \"4.5\")\n",
    "    \"\"\"\n",
    "    if pd.isna(band_value):\n",
    "        return None\n",
    "    \n",
    "    # Convert to string and strip whitespace\n",
    "    band_str = str(band_value).strip()\n",
    "    \n",
    "    # Try to convert to float and back to string for consistent format\n",
    "    try:\n",
    "        band_float = float(band_str)\n",
    "        # Format to one decimal place if it's a decimal, otherwise as integer\n",
    "        if band_float % 1 == 0:\n",
    "            return f\"{band_float:.1f}\"\n",
    "        else:\n",
    "            return str(band_float)\n",
    "    except ValueError:\n",
    "        # If conversion fails, return the stripped string\n",
    "        return band_str\n",
    "\n",
    "# === Load CSV ===\n",
    "input_csv = \"ielts_writing_full_1.csv\"  # replace with your file path\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# === Standardize essays ===\n",
    "df['standardized_essay'] = df['essay'].apply(standardize_essay)\n",
    "\n",
    "# === Extract overall score ===\n",
    "df['overall_score'] = df['evaluation'].apply(extract_overall_score)\n",
    "\n",
    "# === Standardize band column ===\n",
    "df['band'] = df['band'].apply(standardize_band)\n",
    "\n",
    "# === Save to new CSV ===\n",
    "output_csv = \"essays_standardized.csv\"  # replace with desired output path\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Standardized essays saved to {output_csv}\")\n",
    "print(f\"Overall scores extracted from 'evaluation' column\")\n",
    "print(f\"Band scores standardized and whitespace removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82bc155",
   "metadata": {},
   "source": [
    "### 1.2 RST Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RST Features Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def calculate_tree_layout(node, x=0, y=0, level_width=4.5, min_spacing=3):\n",
    "    \"\"\"\n",
    "    Calculate positions for each node in the tree (horizontal layout: left to right).\n",
    "    Returns a dictionary mapping node ids to (x, y) positions.\n",
    "    \"\"\"\n",
    "    positions = {}\n",
    "    heights = {}\n",
    "    \n",
    "    def calculate_height(node):\n",
    "        \"\"\"Calculate the height needed for this subtree\"\"\"\n",
    "        if node is None:\n",
    "            return 0\n",
    "        \n",
    "        has_children = hasattr(node, 'left') and hasattr(node, 'right') and node.left is not None\n",
    "        \n",
    "        if not has_children:\n",
    "            # Leaf node\n",
    "            heights[id(node)] = min_spacing\n",
    "            return min_spacing\n",
    "        \n",
    "        left_height = calculate_height(node.left) if node.left else 0\n",
    "        right_height = calculate_height(node.right) if node.right else 0\n",
    "        total_height = left_height + right_height\n",
    "        heights[id(node)] = total_height\n",
    "        return total_height\n",
    "    \n",
    "    def assign_positions(node, x, y):\n",
    "        \"\"\"Assign x, y positions to each node (horizontal tree)\"\"\"\n",
    "        if node is None:\n",
    "            return\n",
    "        \n",
    "        has_children = hasattr(node, 'left') and hasattr(node, 'right') and node.left is not None\n",
    "        \n",
    "        if not has_children:\n",
    "            # Leaf node\n",
    "            positions[id(node)] = (x, y)\n",
    "            return\n",
    "        \n",
    "        # Internal node\n",
    "        positions[id(node)] = (x, y)\n",
    "        \n",
    "        # Position children (going RIGHT and spreading vertically)\n",
    "        left_height = heights.get(id(node.left), 0) if node.left else 0\n",
    "        right_height = heights.get(id(node.right), 0) if node.right else 0\n",
    "        \n",
    "        if node.left:\n",
    "            left_y = y + right_height / 2\n",
    "            assign_positions(node.left, x + level_width, left_y)\n",
    "        \n",
    "        if node.right:\n",
    "            right_y = y - left_height / 2\n",
    "            assign_positions(node.right, x + level_width, right_y)\n",
    "    \n",
    "    # First calculate heights\n",
    "    calculate_height(node)\n",
    "    # Then assign positions\n",
    "    assign_positions(node, x, y)\n",
    "    \n",
    "    return positions\n",
    "\n",
    "\n",
    "def draw_rst_tree(node, text, output_pdf='rst_tree.pdf', figsize=(24, 16)):\n",
    "    \"\"\"\n",
    "    Draw RST tree horizontally (left to right) and save to PDF.\n",
    "    \n",
    "    Args:\n",
    "        node: Root DiscourseUnit\n",
    "        text: Original text for EDU content\n",
    "        output_pdf: Output PDF filename\n",
    "        figsize: Figure size tuple (width, height) - horizontal layout\n",
    "    \"\"\"\n",
    "    # Calculate layout\n",
    "    positions = calculate_tree_layout(node)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Collections for drawing\n",
    "    edges = []\n",
    "    node_data = []\n",
    "    \n",
    "    def collect_tree_data(node):\n",
    "        \"\"\"Collect all nodes and edges for drawing\"\"\"\n",
    "        if node is None:\n",
    "            return\n",
    "        \n",
    "        node_id = id(node)\n",
    "        pos = positions[node_id]\n",
    "        has_children = hasattr(node, 'left') and hasattr(node, 'right') and node.left is not None\n",
    "        \n",
    "        if has_children:\n",
    "            # Internal node\n",
    "            relation = getattr(node, 'relation', 'Unknown')\n",
    "            nuclearity = getattr(node, 'nuclearity', 'NN')\n",
    "            label = f\"{relation}\\n({nuclearity})\"\n",
    "            \n",
    "            node_data.append({\n",
    "                'pos': pos,\n",
    "                'label': label,\n",
    "                'type': 'internal',\n",
    "                'nuclearity': nuclearity\n",
    "            })\n",
    "            \n",
    "            # Draw edges to children\n",
    "            if node.left:\n",
    "                child_pos = positions[id(node.left)]\n",
    "                left_nuc = nuclearity[0] if len(nuclearity) > 0 else 'N'\n",
    "                edges.append((pos, child_pos, left_nuc))\n",
    "                collect_tree_data(node.left)\n",
    "            \n",
    "            if node.right:\n",
    "                child_pos = positions[id(node.right)]\n",
    "                right_nuc = nuclearity[1] if len(nuclearity) > 1 else 'N'\n",
    "                edges.append((pos, child_pos, right_nuc))\n",
    "                collect_tree_data(node.right)\n",
    "        else:\n",
    "            # Leaf EDU - show only partial text\n",
    "            start = getattr(node, 'start', 0)\n",
    "            end = getattr(node, 'end', 0)\n",
    "            edu_text = text[start:end].strip() if text else ''\n",
    "            \n",
    "            # Show only first 80 characters\n",
    "            if len(edu_text) > 80:\n",
    "                edu_text = edu_text[:27] + \"...\"\n",
    "            \n",
    "            edu_id = getattr(node, 'id', '?')\n",
    "            # Just show the text without EDU label for cleaner look\n",
    "            label = edu_text\n",
    "            \n",
    "            node_data.append({\n",
    "                'pos': pos,\n",
    "                'label': label,\n",
    "                'type': 'leaf',\n",
    "                'edu_id': edu_id\n",
    "            })\n",
    "    \n",
    "    # Collect all data\n",
    "    collect_tree_data(node)\n",
    "    \n",
    "    # Draw edges first (so they appear behind nodes)\n",
    "    for start_pos, end_pos, nuclearity in edges:\n",
    "        # Different line styles for nucleus vs satellite\n",
    "        if nuclearity == 'N':\n",
    "            linestyle = '-'\n",
    "            linewidth = 1\n",
    "            color = 'black'\n",
    "        else:  # 'S'\n",
    "            linestyle = '--'\n",
    "            linewidth = 0.5\n",
    "            color = 'gray'\n",
    "        \n",
    "        ax.plot([start_pos[0], end_pos[0]], \n",
    "                [start_pos[1], end_pos[1]], \n",
    "                linestyle=linestyle, \n",
    "                linewidth=linewidth, \n",
    "                color=color, \n",
    "                zorder=1)\n",
    "    \n",
    "    # Draw nodes (bigger boxes)\n",
    "    for node_info in node_data:\n",
    "        pos = node_info['pos']\n",
    "        label = node_info['label']\n",
    "        \n",
    "        if node_info['type'] == 'internal':\n",
    "            # Internal node - bigger rectangle\n",
    "            box_width = 3.5\n",
    "            box_height = 2\n",
    "            rect = mpatches.FancyBboxPatch(\n",
    "                (pos[0] - box_width/2, pos[1] - box_height/2),\n",
    "                box_width, box_height,\n",
    "                boxstyle=\"round,pad=0.1\",\n",
    "                edgecolor='blue',\n",
    "                facecolor='lightblue',\n",
    "                linewidth=2.5,\n",
    "                zorder=2\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(pos[0], pos[1], label, \n",
    "                   ha='center', va='center', \n",
    "                   fontsize=6, fontweight='bold',\n",
    "                   zorder=3)\n",
    "        else:\n",
    "            # Leaf node - just text, no box\n",
    "            ax.text(pos[0], pos[1], label, \n",
    "                   ha='left', va='center', \n",
    "                   fontsize=6,\n",
    "                   zorder=3)\n",
    "    \n",
    "    # Add legend\n",
    "    internal_patch = mpatches.Rectangle((0, 0), 1, 1, \n",
    "                                       facecolor='lightblue', \n",
    "                                       edgecolor='blue', \n",
    "                                       linewidth=2.5,\n",
    "                                       label='Internal Node (Relation)')\n",
    "    leaf_patch = mpatches.Patch(facecolor='white', \n",
    "                               edgecolor='white', \n",
    "                               linewidth=2.5,\n",
    "                               label='Leaf Node (EDU)')\n",
    "    nucleus_line = mpatches.Patch(color='black', label='Nucleus (solid)')\n",
    "    satellite_line = mpatches.Patch(color='gray', label='Satellite (dashed)')\n",
    "    \n",
    "    ax.legend(handles=[internal_patch, leaf_patch, nucleus_line, satellite_line],\n",
    "             loc='upper right', fontsize=11)\n",
    "    \n",
    "    # Set axis limits with padding\n",
    "    if positions:\n",
    "        x_coords = [pos[0] for pos in positions.values()]\n",
    "        y_coords = [pos[1] for pos in positions.values()]\n",
    "        x_margin = (max(x_coords) - min(x_coords)) * 0.1 + 2\n",
    "        y_margin = (max(y_coords) - min(y_coords)) * 0.1 + 2\n",
    "        ax.set_xlim(min(x_coords) - x_margin, max(x_coords) + x_margin)\n",
    "        ax.set_ylim(min(y_coords) - y_margin, max(y_coords) + y_margin)\n",
    "    \n",
    "    plt.title('RST Tree Visualization', fontsize=18, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save to PDF\n",
    "    with PdfPages(output_pdf) as pdf:\n",
    "        pdf.savefig(fig, bbox_inches='tight')\n",
    "        \n",
    "        # Add metadata\n",
    "        d = pdf.infodict()\n",
    "        d['Title'] = 'RST Tree Visualization'\n",
    "        d['Author'] = 'RST Parser'\n",
    "        d['Subject'] = 'Rhetorical Structure Theory Tree'\n",
    "        d['CreationDate'] = datetime.now()\n",
    "    \n",
    "    plt.close()\n",
    "    print(f\"✓ PDF saved to: {output_pdf}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1941a8",
   "metadata": {},
   "source": [
    "### 1.3 Test and Visualize on an Example Essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c39972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Parser on Example Essay\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from isanlp_rst.parser import Parser\n",
    "from isanlp.annotation_rst import DiscourseUnit\n",
    "import isanlp_rst\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def print_rst_tree(node, depth=0, text=None):\n",
    "    \"\"\"\n",
    "    Pretty-print an RST tree recursively.\n",
    "    \n",
    "    Args:\n",
    "        node: DiscourseUnit object (can be internal node or leaf EDU)\n",
    "        text: full essay text, used to show EDU text\n",
    "        depth: indentation for pretty printing\n",
    "    \"\"\"\n",
    "    indent = \"  \" * depth\n",
    "    \n",
    "    # All nodes are DiscourseUnit objects\n",
    "    # Check if it's an internal node (has children) or leaf (no children)\n",
    "    has_children = hasattr(node, 'left') and hasattr(node, 'right') and node.left is not None\n",
    "    \n",
    "    if has_children:\n",
    "        # Internal node (has left and right children)\n",
    "        relation = getattr(node, 'relation', 'Unknown')\n",
    "        nuclearity = getattr(node, 'nuclearity', 'Unknown')\n",
    "        node_id = getattr(node, 'id', 'N/A')\n",
    "        start = getattr(node, 'start', '?')\n",
    "        end = getattr(node, 'end', '?')\n",
    "        \n",
    "        print(f\"{indent}[Node {node_id}] {relation} ({nuclearity}) | span: ({start}, {end})\")\n",
    "        \n",
    "        # Recurse into children\n",
    "        if node.left is not None:\n",
    "            print(f\"{indent}  ├─ Left:\")\n",
    "            print_rst_tree(node.left, depth + 2, text)\n",
    "        \n",
    "        if node.right is not None:\n",
    "            print(f\"{indent}  └─ Right:\")\n",
    "            print_rst_tree(node.right, depth + 2, text)\n",
    "    \n",
    "    else:\n",
    "        # Leaf EDU (no children)\n",
    "        edu_id = getattr(node, 'id', 'N/A')\n",
    "        start = getattr(node, 'start', 0)\n",
    "        end = getattr(node, 'end', 0)\n",
    "        \n",
    "        if text:\n",
    "            edu_text = text[start:end].strip()\n",
    "            # Truncate long text\n",
    "            if len(edu_text) > 100:\n",
    "                edu_text = edu_text[:97] + \"...\"\n",
    "        else:\n",
    "            edu_text = getattr(node, 'text', '[no text]')\n",
    "            if len(edu_text) > 100:\n",
    "                edu_text = edu_text[:97] + \"...\"\n",
    "        \n",
    "        print(f\"{indent}[EDU {edu_id}] span: ({start}, {end})\")\n",
    "        print(f\"{indent}  └─ \\\"{edu_text}\\\"\")\n",
    "\n",
    "\n",
    "def extract_rst_statistics(node):\n",
    "    \"\"\"\n",
    "    Extract statistics from RST tree.\n",
    "    \n",
    "    Returns:\n",
    "        dict with tree statistics\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'total_nodes': 0,\n",
    "        'total_edus': 0,\n",
    "        'max_depth': 0,\n",
    "        'relations': [],\n",
    "        'nuclearity_counts': {'NN': 0, 'NS': 0, 'SN': 0}\n",
    "    }\n",
    "    \n",
    "    def traverse(node, depth=0):\n",
    "        if node is None:\n",
    "            return\n",
    "            \n",
    "        stats['max_depth'] = max(stats['max_depth'], depth)\n",
    "        \n",
    "        # Check if internal node (has children)\n",
    "        has_children = hasattr(node, 'left') and hasattr(node, 'right') and node.left is not None\n",
    "        \n",
    "        if has_children:\n",
    "            stats['total_nodes'] += 1\n",
    "            relation = getattr(node, 'relation', 'Unknown')\n",
    "            nuclearity = getattr(node, 'nuclearity', 'Unknown')\n",
    "            \n",
    "            stats['relations'].append(relation)\n",
    "            if nuclearity in stats['nuclearity_counts']:\n",
    "                stats['nuclearity_counts'][nuclearity] += 1\n",
    "            \n",
    "            # Recurse\n",
    "            traverse(node.left, depth + 1)\n",
    "            traverse(node.right, depth + 1)\n",
    "        else:\n",
    "            # Leaf EDU\n",
    "            stats['total_edus'] += 1\n",
    "    \n",
    "    traverse(node)\n",
    "    return stats\n",
    "\n",
    "\n",
    "# === Parser setup ===\n",
    "print(\"=\"*80)\n",
    "print(\"RST Parser Setup\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "version = 'rstdt'\n",
    "parser = Parser(\n",
    "    hf_model_name='tchewik/isanlp_rst_v3',\n",
    "    hf_model_version=version,\n",
    "    cuda_device=-1  \n",
    ")\n",
    "\n",
    "print(f\"✓ Parser loaded: {version} model\")\n",
    "print()\n",
    "\n",
    "# === Example text ===\n",
    "text = \"\"\"\n",
    "Preserving the natural environment has become very crucial as manufacturing of consumer products on a large scale has led to environmental disturbance. \n",
    "People's needs have changed over time, which has caused immense damage to the natural surroundings. \n",
    "There are many ways this issue can be controlled. \n",
    "We shall continue to discuss in the next paragraphs.\n",
    "\n",
    "Cutting down trees is the major cause of natural imbalance. \n",
    "In our lives, plants and trees are the major source of fresh air. \n",
    "Trees have been cut down to meet the increased demand for rubber, paper, and other products. \n",
    "The government should impose heavy fines on such people or they should be imprisoned if they cause harm to these natural resources.\n",
    "\n",
    "Another important cause of the extinction of natural resources is the increased demand for land. \n",
    "Mountains, forests, and barren fields are being used to construct huge buildings and malls. \n",
    "Such land should be used for growing or planting more trees instead of selling thousands of acres to MNCs or selfish builders. \n",
    "Educating children at a young age can help preserve these precious resources from extinction.\n",
    "\n",
    "To conclude, civilians and government should go hand in hand to preserve these resources. \n",
    "Instead of overusing natural resources, alternative ways should be found to meet the increased demands of the population.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Essay Text\")\n",
    "print(\"=\"*80)\n",
    "print(text.strip())\n",
    "print()\n",
    "\n",
    "# === Parse and print tree ===\n",
    "print(\"=\"*80)\n",
    "print(\"Parsing Essay...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "res = parser(text)\n",
    "root = res['rst'][0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RST Tree Structure\")\n",
    "print(\"=\"*80)\n",
    "print_rst_tree(root, text=text)\n",
    "\n",
    "# === Extract and display statistics ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RST Tree Statistics\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stats = extract_rst_statistics(root)\n",
    "print(f\"Total Internal Nodes: {stats['total_nodes']}\")\n",
    "print(f\"Total EDUs (Leaf Units): {stats['total_edus']}\")\n",
    "print(f\"Maximum Tree Depth: {stats['max_depth']}\")\n",
    "\n",
    "print(f\"\\nNuclearity Distribution:\")\n",
    "total_nuclearity = sum(stats['nuclearity_counts'].values())\n",
    "if total_nuclearity > 0:\n",
    "    print(f\"  - NN (Nucleus-Nucleus): {stats['nuclearity_counts']['NN']} ({stats['nuclearity_counts']['NN']/total_nuclearity*100:.1f}%)\")\n",
    "    print(f\"  - NS (Nucleus-Satellite): {stats['nuclearity_counts']['NS']} ({stats['nuclearity_counts']['NS']/total_nuclearity*100:.1f}%)\")\n",
    "    print(f\"  - SN (Satellite-Nucleus): {stats['nuclearity_counts']['SN']} ({stats['nuclearity_counts']['SN']/total_nuclearity*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"  No nuclearity data found\")\n",
    "\n",
    "if stats['relations']:\n",
    "    print(f\"\\nRelation Types Found:\")\n",
    "    unique_relations = set(stats['relations'])\n",
    "    for relation in sorted(unique_relations):\n",
    "        count = stats['relations'].count(relation)\n",
    "        percentage = (count / len(stats['relations'])) * 100\n",
    "        print(f\"  - {relation}: {count} ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo relations found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Root Node Details\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ID: {root.id}\")\n",
    "print(f\"Relation: {root.relation}\")\n",
    "print(f\"Nuclearity: {root.nuclearity}\")\n",
    "print(f\"Span: ({root.start}, {root.end})\")\n",
    "print(f\"Has children: {root.left is not None}\")\n",
    "\n",
    "print(\"\\n✓ Analysis complete!\")\n",
    "\n",
    "# After parsing:\n",
    "res = parser(text)\n",
    "root = res['rst'][0]\n",
    "\n",
    "# Generate PDF visualization\n",
    "# Use vertical layout with bigger nodes\n",
    "draw_rst_tree(root, text, output_pdf='rst_tree_visualization.pdf', figsize=(24, 16))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa915926",
   "metadata": {},
   "source": [
    "### 1.4 Extract RST Features for The Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285edd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RST Parser For The Whole Dataset (CSV)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from isanlp_rst.parser import Parser\n",
    "from isanlp.annotation_rst import DiscourseUnit\n",
    "\n",
    "def extract_rst_statistics(node):\n",
    "    \"\"\"\n",
    "    Extract statistics from RST tree.\n",
    "    \n",
    "    Returns:\n",
    "        dict with tree statistics\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'total_nodes': 0,\n",
    "        'total_edus': 0,\n",
    "        'max_depth': 0,\n",
    "        'relations': [],\n",
    "        'nuclearity_counts': {'NN': 0, 'NS': 0, 'SN': 0}\n",
    "    }\n",
    "    \n",
    "    def traverse(node, depth=0):\n",
    "        if node is None:\n",
    "            return\n",
    "            \n",
    "        stats['max_depth'] = max(stats['max_depth'], depth)\n",
    "        \n",
    "        # Check if internal node (has children)\n",
    "        has_children = hasattr(node, 'left') and hasattr(node, 'right') and node.left is not None\n",
    "        \n",
    "        if has_children:\n",
    "            stats['total_nodes'] += 1\n",
    "            relation = getattr(node, 'relation', 'Unknown')\n",
    "            nuclearity = getattr(node, 'nuclearity', 'Unknown')\n",
    "            \n",
    "            stats['relations'].append(relation)\n",
    "            if nuclearity in stats['nuclearity_counts']:\n",
    "                stats['nuclearity_counts'][nuclearity] += 1\n",
    "            \n",
    "            # Recurse\n",
    "            traverse(node.left, depth + 1)\n",
    "            traverse(node.right, depth + 1)\n",
    "        else:\n",
    "            # Leaf EDU\n",
    "            stats['total_edus'] += 1\n",
    "    \n",
    "    traverse(node)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def process_essay_rst(essay_text, parser):\n",
    "    \"\"\"\n",
    "    Process a single essay and return RST statistics as a dict.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if pd.isna(essay_text) or not essay_text.strip():\n",
    "            return {\n",
    "                'NN_count': None, 'NS_count': None, 'SN_count': None,\n",
    "                'NN_ratio': None, 'NS_ratio': None, 'SN_ratio': None,\n",
    "                'main_nuclearity': None,\n",
    "                'relation_counts': None,\n",
    "                'relation_ratios': None,\n",
    "                'main_relation': None,\n",
    "                'total_edus': None,\n",
    "                'max_depth': None\n",
    "            }\n",
    "        \n",
    "        # Parse the essay\n",
    "        res = parser(essay_text)\n",
    "        root = res['rst'][0]\n",
    "        \n",
    "        # Extract statistics\n",
    "        stats = extract_rst_statistics(root)\n",
    "        \n",
    "        # Calculate nuclearity ratios\n",
    "        total_nuclearity = sum(stats['nuclearity_counts'].values())\n",
    "        if total_nuclearity > 0:\n",
    "            nn_ratio = stats['nuclearity_counts']['NN'] / total_nuclearity\n",
    "            ns_ratio = stats['nuclearity_counts']['NS'] / total_nuclearity\n",
    "            sn_ratio = stats['nuclearity_counts']['SN'] / total_nuclearity\n",
    "            \n",
    "            # Find main nuclearity (most common)\n",
    "            main_nuclearity = max(stats['nuclearity_counts'], key=stats['nuclearity_counts'].get)\n",
    "        else:\n",
    "            nn_ratio = ns_ratio = sn_ratio = 0\n",
    "            main_nuclearity = None\n",
    "        \n",
    "        # Calculate relation statistics\n",
    "        if stats['relations']:\n",
    "            from collections import Counter\n",
    "            relation_counter = Counter(stats['relations'])\n",
    "            \n",
    "            # Get counts and ratios as strings\n",
    "            relation_counts = '; '.join([f\"{rel}: {count}\" for rel, count in relation_counter.most_common()])\n",
    "            relation_ratios = '; '.join([f\"{rel}: {count/len(stats['relations']):.3f}\" for rel, count in relation_counter.most_common()])\n",
    "            \n",
    "            # Main relation (most common)\n",
    "            main_relation = relation_counter.most_common(1)[0][0]\n",
    "        else:\n",
    "            relation_counts = None\n",
    "            relation_ratios = None\n",
    "            main_relation = None\n",
    "        \n",
    "        return {\n",
    "            'NN_count': stats['nuclearity_counts']['NN'],\n",
    "            'NS_count': stats['nuclearity_counts']['NS'],\n",
    "            'SN_count': stats['nuclearity_counts']['SN'],\n",
    "            'NN_ratio': round(nn_ratio, 3),\n",
    "            'NS_ratio': round(ns_ratio, 3),\n",
    "            'SN_ratio': round(sn_ratio, 3),\n",
    "            'main_nuclearity': main_nuclearity,\n",
    "            'relation_counts': relation_counts,\n",
    "            'relation_ratios': relation_ratios,\n",
    "            'main_relation': main_relation,\n",
    "            'total_edus': stats['total_edus'],\n",
    "            'max_depth': stats['max_depth']\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing essay: {e}\")\n",
    "        return {\n",
    "            'NN_count': None, 'NS_count': None, 'SN_count': None,\n",
    "            'NN_ratio': None, 'NS_ratio': None, 'SN_ratio': None,\n",
    "            'main_nuclearity': None,\n",
    "            'relation_counts': None,\n",
    "            'relation_ratios': None,\n",
    "            'main_relation': None,\n",
    "            'total_edus': None,\n",
    "            'max_depth': None\n",
    "        }\n",
    "\n",
    "\n",
    "# === Setup ===\n",
    "print(\"=\"*80)\n",
    "print(\"RST Parser Setup\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "version = 'rstdt'\n",
    "parser = Parser(\n",
    "    hf_model_name='tchewik/isanlp_rst_v3',\n",
    "    hf_model_version=version,\n",
    "    cuda_device=-1  \n",
    ")\n",
    "\n",
    "print(f\"✓ Parser loaded: {version} model\")\n",
    "print()\n",
    "\n",
    "# === Load CSV ===\n",
    "input_csv = \"essays_standardized.csv\"\n",
    "print(f\"Loading {input_csv}...\")\n",
    "df = pd.read_csv(input_csv)\n",
    "print(f\"✓ Loaded {len(df)} essays\")\n",
    "print()\n",
    "\n",
    "# === Process each essay ===\n",
    "print(\"=\"*80)\n",
    "print(\"Processing Essays...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for idx, essay in enumerate(df['standardized_essay'], 1):\n",
    "    print(f\"Processing essay {idx}/{len(df)}...\", end='\\r')\n",
    "    result = process_essay_rst(essay, parser)\n",
    "    results.append(result)\n",
    "\n",
    "print(f\"\\n✓ Processed {len(results)} essays\")\n",
    "print()\n",
    "\n",
    "# === Add results to dataframe ===\n",
    "results_df = pd.DataFrame(results)\n",
    "df = pd.concat([df, results_df], axis=1)\n",
    "\n",
    "# === Save to new CSV ===\n",
    "output_csv = \"essays_with_rst.csv\"\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Results saved to {output_csv}\")\n",
    "print(f\"\\nNew columns added:\")\n",
    "print(\"  - NN_count, NS_count, SN_count\")\n",
    "print(\"  - NN_ratio, NS_ratio, SN_ratio\")\n",
    "print(\"  - main_nuclearity\")\n",
    "print(\"  - relation_counts\")\n",
    "print(\"  - relation_ratios\")\n",
    "print(\"  - main_relation\")\n",
    "print(\"  - total_edus\")\n",
    "print(\"  - max_depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df39c3b",
   "metadata": {},
   "source": [
    "### 1.5 Extract Lexical & Syntactic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767bcd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Lexical & Syntactic Features\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import textstat\n",
    "from wordfreq import word_frequency\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def extract_lexical_features(text, nlp):\n",
    "    \"\"\"\n",
    "    Extract Surface and Lexical Features (LR):\n",
    "    - Word count, sentence count, avg sentence length\n",
    "    - Type-token ratio (TTR)\n",
    "    - Lexical diversity metrics\n",
    "    - Readability scores (Flesch Reading Ease, Flesch-Kincaid Grade)\n",
    "    - Word frequency metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if pd.isna(text) or not text.strip():\n",
    "            return {f'lex_{k}': None for k in [\n",
    "                'word_count', 'sent_count', 'avg_sent_len', 'ttr', \n",
    "                'flesch_reading_ease', 'flesch_kincaid_grade',\n",
    "                'avg_word_freq', 'rare_word_ratio'\n",
    "            ]}\n",
    "        \n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Basic counts\n",
    "        words = [token for token in doc if not token.is_punct and not token.is_space]\n",
    "        word_count = len(words)\n",
    "        sent_count = len(list(doc.sents))\n",
    "        avg_sent_len = word_count / sent_count if sent_count > 0 else 0\n",
    "        \n",
    "        # Type-Token Ratio (lexical diversity)\n",
    "        word_texts = [token.text.lower() for token in words]\n",
    "        unique_words = len(set(word_texts))\n",
    "        ttr = unique_words / word_count if word_count > 0 else 0\n",
    "        \n",
    "        # Readability scores\n",
    "        flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "        flesch_kincaid_grade = textstat.flesch_kincaid_grade(text)\n",
    "        \n",
    "        # Word frequency analysis (using wordfreq)\n",
    "        word_freqs = [word_frequency(token.text.lower(), 'en') for token in words]\n",
    "        avg_word_freq = np.mean(word_freqs) if word_freqs else 0\n",
    "        \n",
    "        # Rare word ratio (words with frequency < 1e-5)\n",
    "        rare_words = sum(1 for freq in word_freqs if freq < 1e-5)\n",
    "        rare_word_ratio = rare_words / word_count if word_count > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'lex_word_count': word_count,\n",
    "            'lex_sent_count': sent_count,\n",
    "            'lex_avg_sent_len': round(avg_sent_len, 2),\n",
    "            'lex_ttr': round(ttr, 3),\n",
    "            'lex_flesch_reading_ease': round(flesch_reading_ease, 2),\n",
    "            'lex_flesch_kincaid_grade': round(flesch_kincaid_grade, 2),\n",
    "            'lex_avg_word_freq': round(avg_word_freq, 6),\n",
    "            'lex_rare_word_ratio': round(rare_word_ratio, 3)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in lexical features: {e}\")\n",
    "        return {f'lex_{k}': None for k in [\n",
    "            'word_count', 'sent_count', 'avg_sent_len', 'ttr',\n",
    "            'flesch_reading_ease', 'flesch_kincaid_grade',\n",
    "            'avg_word_freq', 'rare_word_ratio'\n",
    "        ]}\n",
    "\n",
    "\n",
    "def extract_syntactic_features(text, nlp):\n",
    "    \"\"\"\n",
    "    Extract Morphological and Syntactic Complexity (GR):\n",
    "    - POS tag distribution (noun, verb, adj, adv ratios)\n",
    "    - Dependency relation distribution\n",
    "    - Average dependency tree depth\n",
    "    - Subordinate clause count\n",
    "    - Passive voice count\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if pd.isna(text) or not text.strip():\n",
    "            return {f'syn_{k}': None for k in [\n",
    "                'noun_ratio', 'verb_ratio', 'adj_ratio', 'adv_ratio',\n",
    "                'avg_tree_depth', 'subordinate_ratio', 'passive_ratio',\n",
    "                'dep_relation_diversity', 'main_dep_relation'\n",
    "            ]}\n",
    "        \n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Get all tokens (excluding punctuation and spaces)\n",
    "        tokens = [token for token in doc if not token.is_punct and not token.is_space]\n",
    "        token_count = len(tokens)\n",
    "        \n",
    "        if token_count == 0:\n",
    "            return {f'syn_{k}': None for k in [\n",
    "                'noun_ratio', 'verb_ratio', 'adj_ratio', 'adv_ratio',\n",
    "                'avg_tree_depth', 'subordinate_ratio', 'passive_ratio',\n",
    "                'dep_relation_diversity', 'main_dep_relation'\n",
    "            ]}\n",
    "        \n",
    "        # POS tag distribution\n",
    "        pos_counts = Counter([token.pos_ for token in tokens])\n",
    "        noun_ratio = (pos_counts['NOUN'] + pos_counts['PROPN']) / token_count\n",
    "        verb_ratio = pos_counts['VERB'] / token_count\n",
    "        adj_ratio = pos_counts['ADJ'] / token_count\n",
    "        adv_ratio = pos_counts['ADV'] / token_count\n",
    "        \n",
    "        # Dependency relations\n",
    "        dep_relations = [token.dep_ for token in tokens]\n",
    "        dep_counter = Counter(dep_relations)\n",
    "        \n",
    "        # Dependency relation diversity (unique deps / total deps)\n",
    "        dep_relation_diversity = len(set(dep_relations)) / len(dep_relations) if dep_relations else 0\n",
    "        \n",
    "        # Main dependency relation (most common)\n",
    "        main_dep_relation = dep_counter.most_common(1)[0][0] if dep_counter else None\n",
    "        \n",
    "        # Average dependency tree depth\n",
    "        def get_depth(token):\n",
    "            depth = 0\n",
    "            while token.head != token:\n",
    "                depth += 1\n",
    "                token = token.head\n",
    "            return depth\n",
    "        \n",
    "        depths = [get_depth(token) for token in tokens]\n",
    "        avg_tree_depth = np.mean(depths) if depths else 0\n",
    "        \n",
    "        # Subordinate clause count (clauses with 'mark' or 'advcl' dependency)\n",
    "        subordinate_count = sum(1 for token in tokens if token.dep_ in ['mark', 'advcl', 'ccomp', 'xcomp'])\n",
    "        subordinate_ratio = subordinate_count / token_count\n",
    "        \n",
    "        # Passive voice detection (auxiliary 'be' + past participle)\n",
    "        passive_count = sum(1 for token in tokens \n",
    "                          if token.dep_ == 'auxpass' or \n",
    "                          (token.tag_ == 'VBN' and any(child.dep_ == 'auxpass' for child in token.children)))\n",
    "        passive_ratio = passive_count / token_count\n",
    "        \n",
    "        return {\n",
    "            'syn_noun_ratio': round(noun_ratio, 3),\n",
    "            'syn_verb_ratio': round(verb_ratio, 3),\n",
    "            'syn_adj_ratio': round(adj_ratio, 3),\n",
    "            'syn_adv_ratio': round(adv_ratio, 3),\n",
    "            'syn_avg_tree_depth': round(avg_tree_depth, 2),\n",
    "            'syn_subordinate_ratio': round(subordinate_ratio, 3),\n",
    "            'syn_passive_ratio': round(passive_ratio, 3),\n",
    "            'syn_dep_relation_diversity': round(dep_relation_diversity, 3),\n",
    "            'syn_main_dep_relation': main_dep_relation\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in syntactic features: {e}\")\n",
    "        return {f'syn_{k}': None for k in [\n",
    "            'noun_ratio', 'verb_ratio', 'adj_ratio', 'adv_ratio',\n",
    "            'avg_tree_depth', 'subordinate_ratio', 'passive_ratio',\n",
    "            'dep_relation_diversity', 'main_dep_relation'\n",
    "        ]}\n",
    "\n",
    "\n",
    "def process_essay_features(essay_text, nlp):\n",
    "    \"\"\"\n",
    "    Process a single essay and return all lexical and syntactic features.\n",
    "    \"\"\"\n",
    "    lex_features = extract_lexical_features(essay_text, nlp)\n",
    "    syn_features = extract_syntactic_features(essay_text, nlp)\n",
    "    \n",
    "    # Combine all features\n",
    "    return {**lex_features, **syn_features}\n",
    "\n",
    "\n",
    "# === Setup ===\n",
    "print(\"=\"*80)\n",
    "print(\"Feature Extraction Setup\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Loading spaCy model...\")\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "print(\"✓ spaCy model loaded\")\n",
    "print()\n",
    "\n",
    "# === Load CSV ===\n",
    "input_csv = \"essays_with_rst.csv\"\n",
    "print(f\"Loading {input_csv}...\")\n",
    "df = pd.read_csv(input_csv)\n",
    "print(f\"✓ Loaded {len(df)} essays\")\n",
    "print()\n",
    "\n",
    "# === Process each essay ===\n",
    "print(\"=\"*80)\n",
    "print(\"Extracting Features...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for idx, essay in enumerate(df['standardized_essay'], 1):\n",
    "    print(f\"Processing essay {idx}/{len(df)}...\", end='\\r')\n",
    "    result = process_essay_features(essay, nlp)\n",
    "    results.append(result)\n",
    "\n",
    "print(f\"\\n✓ Processed {len(results)} essays\")\n",
    "print()\n",
    "\n",
    "# === Add results to dataframe ===\n",
    "results_df = pd.DataFrame(results)\n",
    "df = pd.concat([df, results_df], axis=1)\n",
    "\n",
    "# === Save to new CSV ===\n",
    "output_csv = \"essays_with_features.csv\"\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Results saved to {output_csv}\")\n",
    "print(f\"\\nLexical Features (LR) added:\")\n",
    "print(\"  - lex_word_count\")\n",
    "print(\"  - lex_sent_count\")\n",
    "print(\"  - lex_avg_sent_len\")\n",
    "print(\"  - lex_ttr (Type-Token Ratio)\")\n",
    "print(\"  - lex_flesch_reading_ease\")\n",
    "print(\"  - lex_flesch_kincaid_grade\")\n",
    "print(\"  - lex_avg_word_freq\")\n",
    "print(\"  - lex_rare_word_ratio\")\n",
    "print(f\"\\nSyntactic Features (GR) added:\")\n",
    "print(\"  - syn_noun_ratio\")\n",
    "print(\"  - syn_verb_ratio\")\n",
    "print(\"  - syn_adj_ratio\")\n",
    "print(\"  - syn_adv_ratio\")\n",
    "print(\"  - syn_avg_tree_depth\")\n",
    "print(\"  - syn_subordinate_ratio\")\n",
    "print(\"  - syn_passive_ratio\")\n",
    "print(\"  - syn_dep_relation_diversity\")\n",
    "print(\"  - syn_main_dep_relation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc7acf",
   "metadata": {},
   "source": [
    "## 2. Systematic Bias Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e3f3a",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c177c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Band Score Discrepancy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Setup ===\n",
    "print(\"=\"*80)\n",
    "print(\"Band Score Bias Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# === Load CSV ===\n",
    "input_csv = \"essays_with_features.csv\"\n",
    "print(f\"Loading {input_csv}...\")\n",
    "df = pd.read_csv(input_csv)\n",
    "print(f\"✓ Loaded {len(df)} essays\")\n",
    "print()\n",
    "\n",
    "# === Handle \"<4\" and convert to numeric ===\n",
    "def calculate_discrepancy(row):\n",
    "    \"\"\"\n",
    "    Calculate discrepancy handling the special \"<4\" case.\n",
    "    - If band is \"<4\" and overall_score < 4: discrepancy = 0 (match)\n",
    "    - If band is \"<4\" and overall_score >= 4: discrepancy = overall_score - 3.5 (treating <4 as ~3.5)\n",
    "    - Otherwise: discrepancy = overall_score - band\n",
    "    \"\"\"\n",
    "    band = str(row['band']).strip()\n",
    "    overall_score = row['overall_score']\n",
    "    \n",
    "    # Handle missing values\n",
    "    if pd.isna(band) or pd.isna(overall_score):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        overall_score = float(overall_score)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    # Handle \"<4\" case\n",
    "    if band == \"<4\":\n",
    "        if overall_score < 4:\n",
    "            return 0  # Match\n",
    "        else:\n",
    "            return 1  # Not a match, assign 1 discrepancy\n",
    "    \n",
    "    # Normal numeric band\n",
    "    try:\n",
    "        band_numeric = float(band)\n",
    "        return overall_score - band_numeric\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply discrepancy calculation\n",
    "df['discrepancy'] = df.apply(calculate_discrepancy, axis=1)\n",
    "df['abs_discrepancy'] = df['discrepancy'].abs()\n",
    "\n",
    "# Create numeric versions for grouping\n",
    "def band_to_numeric(band):\n",
    "    \"\"\"Convert band to numeric, treating '<4' as 3.5 for grouping purposes.\"\"\"\n",
    "    band = str(band).strip()\n",
    "    if band == \"<4\":\n",
    "        return 3.5\n",
    "    try:\n",
    "        return float(band)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['band_numeric'] = df['band'].apply(band_to_numeric)\n",
    "df['overall_score_numeric'] = pd.to_numeric(df['overall_score'], errors='coerce')\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_clean = df.dropna(subset=['band_numeric', 'overall_score_numeric', 'discrepancy'])\n",
    "print(f\"✓ Cleaned data: {len(df_clean)} essays with valid band and overall_score\")\n",
    "print()\n",
    "\n",
    "# Count special \"<4\" cases\n",
    "less_than_4_count = (df_clean['band'].astype(str).str.strip() == \"<4\").sum()\n",
    "if less_than_4_count > 0:\n",
    "    print(f\"Note: Found {less_than_4_count} essays with band '<4'\")\n",
    "    print()\n",
    "\n",
    "# === Group by band score ===\n",
    "print(\"=\"*80)\n",
    "print(\"Discrepancy Analysis by Band Score\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a display band that shows \"<4\" properly\n",
    "df_clean['band_display'] = df_clean['band'].astype(str).str.strip()\n",
    "\n",
    "grouped = df_clean.groupby('band_display').agg({\n",
    "    'overall_score_numeric': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'discrepancy': ['mean', 'std', 'min', 'max'],\n",
    "    'abs_discrepancy': ['mean', 'max']\n",
    "}).round(3)\n",
    "\n",
    "# Flatten column names\n",
    "grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "grouped.columns = ['essay_count', 'avg_overall_score', 'std_overall_score', \n",
    "                   'min_overall_score', 'max_overall_score',\n",
    "                   'avg_discrepancy', 'std_discrepancy', 'min_discrepancy', \n",
    "                   'max_discrepancy', 'avg_abs_discrepancy', 'max_abs_discrepancy']\n",
    "\n",
    "# Sort by band (with <4 first)\n",
    "grouped = grouped.sort_index(key=lambda x: x.map(lambda v: -1 if v == '<4' else float(v)))\n",
    "\n",
    "print(grouped)\n",
    "print()\n",
    "\n",
    "# === Overall statistics ===\n",
    "print(\"=\"*80)\n",
    "print(\"Overall Discrepancy Statistics\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total essays analyzed: {len(df_clean)}\")\n",
    "print(f\"Mean discrepancy (overall_score - band): {df_clean['discrepancy'].mean():.3f}\")\n",
    "print(f\"Std discrepancy: {df_clean['discrepancy'].std():.3f}\")\n",
    "print(f\"Mean absolute discrepancy: {df_clean['abs_discrepancy'].mean():.3f}\")\n",
    "print(f\"Max absolute discrepancy: {df_clean['abs_discrepancy'].max():.3f}\")\n",
    "print()\n",
    "\n",
    "# Count essays by discrepancy range\n",
    "print(\"Discrepancy Distribution:\")\n",
    "print(f\"  Perfect match (discrepancy = 0): {(df_clean['discrepancy'] == 0).sum()} essays ({(df_clean['discrepancy'] == 0).sum()/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"  Small discrepancy (|disc| <= 0.5): {(df_clean['abs_discrepancy'] <= 0.5).sum()} essays ({(df_clean['abs_discrepancy'] <= 0.5).sum()/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"  Medium discrepancy (0.5 < |disc| <= 1.0): {((df_clean['abs_discrepancy'] > 0.5) & (df_clean['abs_discrepancy'] <= 1.0)).sum()} essays ({((df_clean['abs_discrepancy'] > 0.5) & (df_clean['abs_discrepancy'] <= 1.0)).sum()/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"  Large discrepancy (|disc| > 1.0): {(df_clean['abs_discrepancy'] > 1.0).sum()} essays ({(df_clean['abs_discrepancy'] > 1.0).sum()/len(df_clean)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Bias direction\n",
    "print(\"Bias Direction:\")\n",
    "overestimated = (df_clean['discrepancy'] > 0).sum()\n",
    "underestimated = (df_clean['discrepancy'] < 0).sum()\n",
    "perfect = (df_clean['discrepancy'] == 0).sum()\n",
    "print(f\"  Overall_score > Band (overestimated): {overestimated} essays ({overestimated/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"  Overall_score < Band (underestimated): {underestimated} essays ({underestimated/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"  Overall_score = Band (perfect match): {perfect} essays ({perfect/len(df_clean)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# === Find essays with largest discrepancies ===\n",
    "print(\"=\"*80)\n",
    "print(\"Top 10 Essays with Largest Discrepancies\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "largest_disc = df_clean.nlargest(10, 'abs_discrepancy')[['band_display', 'overall_score_numeric', 'discrepancy', 'abs_discrepancy']]\n",
    "largest_disc.columns = ['Band', 'Overall_Score', 'Discrepancy', 'Abs_Discrepancy']\n",
    "print(largest_disc.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# === Save results ===\n",
    "output_csv = \"essays_with_bias_analysis.csv\"\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Save grouped statistics\n",
    "grouped_output = \"band_discrepancy_summary.csv\"\n",
    "grouped.to_csv(grouped_output)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Full results saved to {output_csv}\")\n",
    "print(f\"✓ Grouped statistics saved to {grouped_output}\")\n",
    "print(f\"\\nNew columns added:\")\n",
    "print(\"  - band_numeric (numeric version of band, '<4' treated as 3.5)\")\n",
    "print(\"  - overall_score_numeric (numeric version of overall_score)\")\n",
    "print(\"  - discrepancy (calculated with '<4' handling)\")\n",
    "print(\"  - abs_discrepancy (absolute value of discrepancy)\")\n",
    "print(f\"\\nSpecial '<4' handling:\")\n",
    "print(f\"  - If overall_score < 4: discrepancy = 0 (match)\")\n",
    "print(f\"  - If overall_score >= 4: discrepancy = 1 (not match)\")\n",
    "#= overall_score (AI) - band for each essay (Human)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f11495",
   "metadata": {},
   "source": [
    "### 2.2 Systematic Bias Detection - Group-Based Analysis - Predictive Models Building - SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b46086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Systematic Bias Detection - Group-Based Analysis\n",
      "================================================================================\n",
      "Loading essays_with_bias_analysis.csv...\n",
      "✓ Loaded 9800 essays\n",
      "\n",
      "================================================================================\n",
      "Processing RST Relation Ratios\n",
      "================================================================================\n",
      "Found 18 unique RST relations\n",
      "Created 16 relation ratio features\n",
      "\n",
      "Using 33 safe features\n",
      "================================================================================\n",
      "Feature Selection\n",
      "================================================================================\n",
      "RST features: 19\n",
      "Lexical features: 6\n",
      "Syntactic features: 8\n",
      "Total features: 33\n",
      "\n",
      "================================================================================\n",
      "Creating Band Interaction Features\n",
      "================================================================================\n",
      "Creating interactions for key features...\n",
      "✓ Created 12 interaction features\n",
      "  Examples: lex_ttr_x_band, lex_avg_word_freq_x_band, syn_dep_relation_diversity_x_band...\n",
      "Total features after interactions: 45\n",
      "================================================================================\n",
      "STEP 1: Detecting Systematic Bias Across Feature Groups\n",
      "================================================================================\n",
      "Bias = Systematic difference in outcomes between subgroups\n",
      "\n",
      "Analyzing 9800 essays with complete features\n",
      "\n",
      "\n",
      "============================================================\n",
      "Band 3.5 (n=567)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Band 4.0 (n=580)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Band 4.5 (n=640)\n",
      "============================================================\n",
      "  ✓ SYSTEMATIC BIAS: lex_rare_word_ratio\n",
      "    Magnitude: 0.158 points (p=0.0190, d=-0.227)\n",
      "    Direction: Low lex_rare_word_ratio → under-scored by 0.158; High lex_rare_word_ratio → over-scored by 0.158\n",
      "    Low: -0.060, Medium: 0.086, High: 0.099\n",
      "    Disadvantaged (underscored): Low\n",
      "    Advantaged (overscored): High\n",
      "\n",
      "============================================================\n",
      "Band 5.0 (n=1046)\n",
      "============================================================\n",
      "  ✓ SYSTEMATIC BIAS: lex_flesch_reading_ease\n",
      "    Magnitude: 0.284 points (p=0.0004, d=0.269)\n",
      "    Direction: High lex_flesch_reading_ease → under-scored by 0.284; Low lex_flesch_reading_ease → over-scored by 0.284\n",
      "    Low: 0.426, Medium: 0.279, High: 0.142\n",
      "    Disadvantaged (underscored): High\n",
      "    Advantaged (overscored): Low\n",
      "  ✓ SYSTEMATIC BIAS: lex_flesch_kincaid_grade\n",
      "    Magnitude: 0.313 points (p=0.0001, d=-0.302)\n",
      "    Direction: Low lex_flesch_kincaid_grade → under-scored by 0.313; High lex_flesch_kincaid_grade → over-scored by 0.313\n",
      "    Low: 0.125, Medium: 0.284, High: 0.438\n",
      "    Disadvantaged (underscored): Low\n",
      "    Advantaged (overscored): High\n",
      "  ✓ SYSTEMATIC BIAS: syn_verb_ratio\n",
      "    Magnitude: 0.168 points (p=0.0260, d=0.167)\n",
      "    Direction: High syn_verb_ratio → under-scored by 0.168; Low syn_verb_ratio → over-scored by 0.168\n",
      "    Low: 0.353, Medium: 0.304, High: 0.185\n",
      "    Disadvantaged (underscored): High\n",
      "    Advantaged (overscored): Low\n",
      "  ✓ SYSTEMATIC BIAS: syn_avg_tree_depth\n",
      "    Magnitude: 0.196 points (p=0.0128, d=-0.188)\n",
      "    Direction: Low syn_avg_tree_depth → under-scored by 0.196; High syn_avg_tree_depth → over-scored by 0.196\n",
      "    Low: 0.157, Medium: 0.341, High: 0.353\n",
      "    Disadvantaged (underscored): Low\n",
      "    Advantaged (overscored): High\n",
      "  ✓ SYSTEMATIC BIAS: syn_subordinate_ratio\n",
      "    Magnitude: 0.232 points (p=0.0032, d=0.224)\n",
      "    Direction: High syn_subordinate_ratio → under-scored by 0.232; Low syn_subordinate_ratio → over-scored by 0.232\n",
      "    Low: 0.390, Medium: 0.290, High: 0.157\n",
      "    Disadvantaged (underscored): High\n",
      "    Advantaged (overscored): Low\n",
      "  ✓ SYSTEMATIC BIAS: syn_passive_ratio\n",
      "    Magnitude: 0.168 points (p=0.0333, d=-0.162)\n",
      "    Direction: Low syn_passive_ratio → under-scored by 0.168; High syn_passive_ratio → over-scored by 0.168\n",
      "    Low: 0.195, Medium: 0.291, High: 0.363\n",
      "    Disadvantaged (underscored): Low\n",
      "    Advantaged (overscored): High\n",
      "\n",
      "============================================================\n",
      "Band 5.5 (n=956)\n",
      "============================================================\n",
      "  ✓ SYSTEMATIC BIAS: lex_avg_sent_len\n",
      "    Magnitude: 0.121 points (p=0.0012, d=-0.258)\n",
      "    Direction: Low lex_avg_sent_len → under-scored by 0.121; High lex_avg_sent_len → over-scored by 0.121\n",
      "    Low: 0.033, Medium: 0.075, High: 0.153\n",
      "    Disadvantaged (underscored): Low\n",
      "    Advantaged (overscored): High\n",
      "  ✓ SYSTEMATIC BIAS: lex_flesch_reading_ease\n",
      "    Magnitude: 0.141 points (p=0.0002, d=0.298)\n",
      "    Direction: High lex_flesch_reading_ease → under-scored by 0.141; Low lex_flesch_reading_ease → over-scored by 0.141\n",
      "    Low: 0.175, Medium: 0.050, High: 0.034\n",
      "    Disadvantaged (underscored): High\n",
      "    Advantaged (overscored): Low\n",
      "  ✓ SYSTEMATIC BIAS: lex_flesch_kincaid_grade\n",
      "    Magnitude: 0.156 points (p=0.0001, d=-0.323)\n",
      "    Direction: Low lex_flesch_kincaid_grade → under-scored by 0.156; High lex_flesch_kincaid_grade → over-scored by 0.156\n",
      "    Low: 0.014, Medium: 0.077, High: 0.170\n",
      "    Disadvantaged (underscored): Low\n",
      "    Advantaged (overscored): High\n",
      "  ✓ SYSTEMATIC BIAS: syn_verb_ratio\n",
      "    Magnitude: 0.104 points (p=0.0165, d=0.189)\n",
      "    Direction: High syn_verb_ratio → under-scored by 0.104; Low syn_verb_ratio → over-scored by 0.104\n",
      "    Low: 0.139, Medium: 0.082, High: 0.035\n",
      "    Disadvantaged (underscored): High\n",
      "    Advantaged (overscored): Low\n",
      "  ✓ SYSTEMATIC BIAS: syn_avg_tree_depth\n",
      "    Magnitude: 0.113 points (p=0.0049, d=-0.226)\n",
      "    Direction: Low syn_avg_tree_depth → under-scored by 0.113; High syn_avg_tree_depth → over-scored by 0.113\n",
      "    Low: 0.021, Medium: 0.108, High: 0.135\n",
      "    Disadvantaged (underscored): Low\n",
      "    Advantaged (overscored): High\n",
      "\n",
      "============================================================\n",
      "Band 6.0 (n=1142)\n",
      "============================================================\n",
      "  ✓ SYSTEMATIC BIAS: lex_flesch_reading_ease\n",
      "    Magnitude: 0.097 points (p=0.0179, d=0.172)\n",
      "    Direction: High lex_flesch_reading_ease → under-scored by 0.097; Low lex_flesch_reading_ease → over-scored by 0.097\n",
      "    Low: 0.092, Medium: 0.030, High: -0.005\n",
      "    Disadvantaged (underscored): High\n",
      "    Advantaged (overscored): Low\n",
      "\n",
      "============================================================\n",
      "Band 6.5 (n=1168)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Band 7.0 (n=1355)\n",
      "============================================================\n",
      "  ✓ SYSTEMATIC BIAS: lex_flesch_reading_ease\n",
      "    Magnitude: 0.130 points (p=0.0001, d=0.256)\n",
      "    Direction: High lex_flesch_reading_ease → under-scored by 0.130; Low lex_flesch_reading_ease → over-scored by 0.130\n",
      "    Low: -0.083, Medium: -0.160, High: -0.212\n",
      "    Disadvantaged (underscored): High\n",
      "    Advantaged (overscored): Low\n",
      "  ✓ SYSTEMATIC BIAS: lex_flesch_kincaid_grade\n",
      "    Magnitude: 0.120 points (p=0.0004, d=-0.235)\n",
      "    Direction: Low lex_flesch_kincaid_grade → under-scored by 0.120; High lex_flesch_kincaid_grade → over-scored by 0.120\n",
      "    Low: -0.224, Medium: -0.127, High: -0.104\n",
      "    Disadvantaged (underscored): Low\n",
      "    Advantaged (overscored): High\n",
      "  ✓ SYSTEMATIC BIAS: lex_rare_word_ratio\n",
      "    Magnitude: 0.149 points (p=0.0000, d=-0.277)\n",
      "    Direction: Low lex_rare_word_ratio → under-scored by 0.149; High lex_rare_word_ratio → over-scored by 0.149\n",
      "    Low: -0.220, Medium: -0.160, High: -0.071\n",
      "    Disadvantaged (underscored): Low\n",
      "    Advantaged (overscored): High\n",
      "  ✓ SYSTEMATIC BIAS: syn_noun_ratio\n",
      "    Magnitude: 0.102 points (p=0.0026, d=-0.199)\n",
      "    Direction: Low syn_noun_ratio → under-scored by 0.102; High syn_noun_ratio → over-scored by 0.102\n",
      "    Low: -0.207, Medium: -0.139, High: -0.104\n",
      "    Disadvantaged (underscored): Low\n",
      "    Advantaged (overscored): High\n",
      "  ✓ SYSTEMATIC BIAS: syn_adj_ratio\n",
      "    Magnitude: 0.150 points (p=0.0000, d=-0.279)\n",
      "    Direction: Low syn_adj_ratio → under-scored by 0.150; High syn_adj_ratio → over-scored by 0.150\n",
      "    Low: -0.249, Medium: -0.103, High: -0.099\n",
      "    Disadvantaged (underscored): Low\n",
      "    Advantaged (overscored): High\n",
      "  ✓ SYSTEMATIC BIAS: syn_dep_relation_diversity\n",
      "    Magnitude: 0.093 points (p=0.0055, d=0.186)\n",
      "    Direction: High syn_dep_relation_diversity → under-scored by 0.093; Low syn_dep_relation_diversity → over-scored by 0.093\n",
      "    Low: -0.095, Medium: -0.175, High: -0.188\n",
      "    Disadvantaged (underscored): High\n",
      "    Advantaged (overscored): Low\n",
      "\n",
      "============================================================\n",
      "Band 7.5 (n=1105)\n",
      "============================================================\n",
      "  ✓ SYSTEMATIC BIAS: lex_rare_word_ratio\n",
      "    Magnitude: 0.102 points (p=0.0000, d=-0.320)\n",
      "    Direction: Low lex_rare_word_ratio → under-scored by 0.102; High lex_rare_word_ratio → over-scored by 0.102\n",
      "    Low: -0.105, Medium: -0.087, High: -0.003\n",
      "    Disadvantaged (underscored): Low\n",
      "    Advantaged (overscored): High\n",
      "\n",
      "============================================================\n",
      "Band 8.0 (n=700)\n",
      "============================================================\n",
      "  ✓ SYSTEMATIC BIAS: lex_flesch_reading_ease\n",
      "    Magnitude: 0.208 points (p=0.0025, d=0.281)\n",
      "    Direction: High lex_flesch_reading_ease → under-scored by 0.208; Low lex_flesch_reading_ease → over-scored by 0.208\n",
      "    Low: -0.374, Medium: -0.526, High: -0.582\n",
      "    Disadvantaged (underscored): High\n",
      "    Advantaged (overscored): Low\n",
      "  ✓ SYSTEMATIC BIAS: lex_flesch_kincaid_grade\n",
      "    Magnitude: 0.156 points (p=0.0253, d=-0.208)\n",
      "    Direction: Low lex_flesch_kincaid_grade → under-scored by 0.156; High lex_flesch_kincaid_grade → over-scored by 0.156\n",
      "    Low: -0.564, Medium: -0.509, High: -0.408\n",
      "    Disadvantaged (underscored): Low\n",
      "    Advantaged (overscored): High\n",
      "  ✓ SYSTEMATIC BIAS: syn_adj_ratio\n",
      "    Magnitude: 0.190 points (p=0.0076, d=-0.248)\n",
      "    Direction: Low syn_adj_ratio → under-scored by 0.190; High syn_adj_ratio → over-scored by 0.190\n",
      "    Low: -0.597, Medium: -0.470, High: -0.407\n",
      "    Disadvantaged (underscored): Low\n",
      "    Advantaged (overscored): High\n",
      "\n",
      "============================================================\n",
      "Band 8.5 (n=433)\n",
      "============================================================\n",
      "  ✓ SYSTEMATIC BIAS: lex_flesch_reading_ease\n",
      "    Magnitude: 0.142 points (p=0.0311, d=0.255)\n",
      "    Direction: High lex_flesch_reading_ease → under-scored by 0.142; Medium lex_flesch_reading_ease → over-scored by 0.142\n",
      "    Low: -0.066, Medium: -0.038, High: -0.181\n",
      "    Disadvantaged (underscored): High\n",
      "    Advantaged (overscored): Medium\n",
      "  ✓ SYSTEMATIC BIAS: lex_flesch_kincaid_grade\n",
      "    Magnitude: 0.134 points (p=0.0393, d=-0.244)\n",
      "    Direction: Low lex_flesch_kincaid_grade → under-scored by 0.134; Medium lex_flesch_kincaid_grade → over-scored by 0.134\n",
      "    Low: -0.176, Medium: -0.042, High: -0.066\n",
      "    Disadvantaged (underscored): Low\n",
      "    Advantaged (overscored): Medium\n",
      "\n",
      "============================================================\n",
      "Band 9.0 (n=108)\n",
      "============================================================\n",
      "  ✓ SYSTEMATIC BIAS: lex_flesch_reading_ease\n",
      "    Magnitude: 0.694 points (p=0.0244, d=0.542)\n",
      "    Direction: Medium lex_flesch_reading_ease → under-scored by 0.694; Low lex_flesch_reading_ease → over-scored by 0.694\n",
      "    Low: -0.833, Medium: -1.528, High: -1.333\n",
      "    Disadvantaged (underscored): Medium\n",
      "    Advantaged (overscored): Low\n",
      "\n",
      "================================================================================\n",
      "Systematic Bias Detection Summary\n",
      "================================================================================\n",
      "\n",
      "Applying FDR correction for multiple comparisons...\n",
      "  Original significant (p < 0.05): 26\n",
      "  FDR-significant (q < 0.05): 26\n",
      "\n",
      "✓ Found 26 cases of systematic bias\n",
      "\n",
      "Top 10 Features with Strongest Systematic Bias:\n",
      " band                  feature  bias_magnitude  cohens_d  p_value  p_value_fdr                                                                                                  direction\n",
      "  9.0  lex_flesch_reading_ease        0.694444  0.542326 0.024380     0.029375 Medium lex_flesch_reading_ease → under-scored by 0.694; Low lex_flesch_reading_ease → over-scored by 0.694\n",
      "  5.0 lex_flesch_kincaid_grade        0.312862 -0.301725 0.000074     0.000382 Low lex_flesch_kincaid_grade → under-scored by 0.313; High lex_flesch_kincaid_grade → over-scored by 0.313\n",
      "  5.0  lex_flesch_reading_ease        0.283668  0.268857 0.000409     0.001269   High lex_flesch_reading_ease → under-scored by 0.284; Low lex_flesch_reading_ease → over-scored by 0.284\n",
      "  5.0    syn_subordinate_ratio        0.232317  0.224439 0.003200     0.006400       High syn_subordinate_ratio → under-scored by 0.232; Low syn_subordinate_ratio → over-scored by 0.232\n",
      "  8.0  lex_flesch_reading_ease        0.207613  0.280919 0.002537     0.005694   High lex_flesch_reading_ease → under-scored by 0.208; Low lex_flesch_reading_ease → over-scored by 0.208\n",
      "  5.0       syn_avg_tree_depth        0.195644 -0.187822 0.012827     0.019617             Low syn_avg_tree_depth → under-scored by 0.196; High syn_avg_tree_depth → over-scored by 0.196\n",
      "  8.0            syn_adj_ratio        0.189618 -0.247539 0.007644     0.012422                       Low syn_adj_ratio → under-scored by 0.190; High syn_adj_ratio → over-scored by 0.190\n",
      "  5.0           syn_verb_ratio        0.168268  0.167434 0.025986     0.029375                     High syn_verb_ratio → under-scored by 0.168; Low syn_verb_ratio → over-scored by 0.168\n",
      "  5.0        syn_passive_ratio        0.167971 -0.161944 0.033284     0.034615               Low syn_passive_ratio → under-scored by 0.168; High syn_passive_ratio → over-scored by 0.168\n",
      "  4.5      lex_rare_word_ratio        0.158225 -0.226876 0.018978     0.024672           Low lex_rare_word_ratio → under-scored by 0.158; High lex_rare_word_ratio → over-scored by 0.158\n",
      "\n",
      "Effect Size Distribution:\n",
      "  Small (|d| < 0.5):   25 (96.2%)\n",
      "  Medium (0.5 ≤ |d| < 0.8): 1 (3.8%)\n",
      "  Large (|d| ≥ 0.8):   0 (0.0%)\n",
      "\n",
      "✓ Saved detailed findings to systematic_bias_findings.csv\n",
      "\n",
      "================================================================================\n",
      "STEP 2: Labeling Essays by Systematic Bias Group Membership\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Top 3 Most Biased Features by Band\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Band 3.5: No systematic bias detected\n",
      "\n",
      "Band 4.0: No systematic bias detected\n",
      "\n",
      "Band 4.5:\n",
      "  1. lex_rare_word_ratio\n",
      "     Bias Magnitude:      0.1582\n",
      "     Cohen's d:           -0.2269\n",
      "     p-value:             0.0190\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): Low\n",
      "     Advantaged (overscored):     High\n",
      "\n",
      "Band 5.0:\n",
      "  1. lex_flesch_kincaid_grade\n",
      "     Bias Magnitude:      0.3129\n",
      "     Cohen's d:           -0.3017\n",
      "     p-value:             0.0001\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): Low\n",
      "     Advantaged (overscored):     High\n",
      "  2. lex_flesch_reading_ease\n",
      "     Bias Magnitude:      0.2837\n",
      "     Cohen's d:           0.2689\n",
      "     p-value:             0.0004\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): High\n",
      "     Advantaged (overscored):     Low\n",
      "  3. syn_subordinate_ratio\n",
      "     Bias Magnitude:      0.2323\n",
      "     Cohen's d:           0.2244\n",
      "     p-value:             0.0032\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): High\n",
      "     Advantaged (overscored):     Low\n",
      "\n",
      "Band 5.5:\n",
      "  1. lex_flesch_kincaid_grade\n",
      "     Bias Magnitude:      0.1562\n",
      "     Cohen's d:           -0.3234\n",
      "     p-value:             0.0001\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): Low\n",
      "     Advantaged (overscored):     High\n",
      "  2. lex_flesch_reading_ease\n",
      "     Bias Magnitude:      0.1405\n",
      "     Cohen's d:           0.2978\n",
      "     p-value:             0.0002\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): High\n",
      "     Advantaged (overscored):     Low\n",
      "  3. lex_avg_sent_len\n",
      "     Bias Magnitude:      0.1206\n",
      "     Cohen's d:           -0.2576\n",
      "     p-value:             0.0012\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): Low\n",
      "     Advantaged (overscored):     High\n",
      "\n",
      "Band 6.0:\n",
      "  1. lex_flesch_reading_ease\n",
      "     Bias Magnitude:      0.0971\n",
      "     Cohen's d:           0.1721\n",
      "     p-value:             0.0179\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): High\n",
      "     Advantaged (overscored):     Low\n",
      "\n",
      "Band 6.5: No systematic bias detected\n",
      "\n",
      "Band 7.0:\n",
      "  1. syn_adj_ratio\n",
      "     Bias Magnitude:      0.1498\n",
      "     Cohen's d:           -0.2789\n",
      "     p-value:             0.0000\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): Low\n",
      "     Advantaged (overscored):     High\n",
      "  2. lex_rare_word_ratio\n",
      "     Bias Magnitude:      0.1492\n",
      "     Cohen's d:           -0.2774\n",
      "     p-value:             0.0000\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): Low\n",
      "     Advantaged (overscored):     High\n",
      "  3. lex_flesch_reading_ease\n",
      "     Bias Magnitude:      0.1296\n",
      "     Cohen's d:           0.2558\n",
      "     p-value:             0.0001\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): High\n",
      "     Advantaged (overscored):     Low\n",
      "\n",
      "Band 7.5:\n",
      "  1. lex_rare_word_ratio\n",
      "     Bias Magnitude:      0.1018\n",
      "     Cohen's d:           -0.3200\n",
      "     p-value:             0.0000\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): Low\n",
      "     Advantaged (overscored):     High\n",
      "\n",
      "Band 8.0:\n",
      "  1. lex_flesch_reading_ease\n",
      "     Bias Magnitude:      0.2076\n",
      "     Cohen's d:           0.2809\n",
      "     p-value:             0.0025\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): High\n",
      "     Advantaged (overscored):     Low\n",
      "  2. syn_adj_ratio\n",
      "     Bias Magnitude:      0.1896\n",
      "     Cohen's d:           -0.2475\n",
      "     p-value:             0.0076\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): Low\n",
      "     Advantaged (overscored):     High\n",
      "  3. lex_flesch_kincaid_grade\n",
      "     Bias Magnitude:      0.1560\n",
      "     Cohen's d:           -0.2075\n",
      "     p-value:             0.0253\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): Low\n",
      "     Advantaged (overscored):     High\n",
      "\n",
      "Band 8.5:\n",
      "  1. lex_flesch_reading_ease\n",
      "     Bias Magnitude:      0.1424\n",
      "     Cohen's d:           0.2549\n",
      "     p-value:             0.0311\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): High\n",
      "     Advantaged (overscored):     Medium\n",
      "  2. lex_flesch_kincaid_grade\n",
      "     Bias Magnitude:      0.1342\n",
      "     Cohen's d:           -0.2436\n",
      "     p-value:             0.0393\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): Low\n",
      "     Advantaged (overscored):     Medium\n",
      "\n",
      "Band 9.0:\n",
      "  1. lex_flesch_reading_ease\n",
      "     Bias Magnitude:      0.6944\n",
      "     Cohen's d:           0.5423\n",
      "     p-value:             0.0244\n",
      "     p-value (FDR):       nan\n",
      "     Disadvantaged (underscored): Medium\n",
      "     Advantaged (overscored):     Low\n",
      "\n",
      "✓ Labeled essays based on systematic bias group membership\n",
      "  Systematically biased: 6069 (61.9%)\n",
      "  Not systematically biased: 3731 (38.1%)\n",
      "\n",
      "================================================================================\n",
      "STEP 3: Building Predictive Model\n",
      "================================================================================\n",
      "Goal: Predict which essays belong to systematically disadvantaged groups\n",
      "\n",
      "\n",
      "Features for modeling: 39 (down from 45)\n",
      "Total features: 63\n",
      "Target balance: 61.9% systematically biased\n",
      "\n",
      "Training set: 5880 (60.0%)\n",
      "Validation set: 1960 (20.0%)\n",
      "Test set: 1960 (20.0%)\n",
      "\n",
      "================================================================================\n",
      "Training Models\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Model: Random Forest\n",
      "============================================================\n",
      "  Accuracy:  0.8561\n",
      "  Precision: 0.8149\n",
      "  Recall:    0.9934\n",
      "  F1-Score:  0.8953\n",
      "  ROC-AUC:   0.9020\n",
      "\n",
      "============================================================\n",
      "Model: XGBoost\n",
      "============================================================\n",
      "  Accuracy:  0.8617\n",
      "  Precision: 0.8485\n",
      "  Recall:    0.9456\n",
      "  F1-Score:  0.8944\n",
      "  ROC-AUC:   0.9128\n",
      "\n",
      "============================================================\n",
      "Model: Logistic Regression\n",
      "============================================================\n",
      "  Accuracy:  0.8250\n",
      "  Precision: 0.8158\n",
      "  Recall:    0.9267\n",
      "  F1-Score:  0.8677\n",
      "  ROC-AUC:   0.8903\n",
      "\n",
      "============================================================\n",
      "Model: SVM\n",
      "============================================================\n",
      "  Accuracy:  0.8500\n",
      "  Precision: 0.8217\n",
      "  Recall:    0.9679\n",
      "  F1-Score:  0.8888\n",
      "  ROC-AUC:   0.8928\n",
      "\n",
      "================================================================================\n",
      "BEST MODEL\n",
      "================================================================================\n",
      "Model: XGBoost\n",
      "Test F1: 0.8944\n",
      "Test AUC: 0.9128\n",
      "\n",
      "================================================================================\n",
      "Hyperparameter Tuning for XGBoost (Optuna)\n",
      "================================================================================\n",
      "\n",
      "Starting Optuna optimization...\n",
      "  Trials: 100\n",
      "  Objective: Maximize F1-Score on validation set\n",
      "  Current best F1: 0.8944\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa6d9a296d8412ea9e6f7c4d418dddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Optimization complete!\n",
      "  Best validation F1-Score: 0.9111\n",
      "  Improvement: 0.0167\n",
      "\n",
      "Best hyperparameters:\n",
      "  n_estimators: 350\n",
      "  max_depth: 9\n",
      "  learning_rate: 0.03\n",
      "  subsample: 0.8\n",
      "  colsample_bytree: 0.8\n",
      "  min_child_weight: 1\n",
      "  gamma: 0.5\n",
      "  reg_alpha: 0.1\n",
      "  reg_lambda: 0.8\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training Final Tuned XGBoost Model\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Tuned Model Performance on Test Set:\n",
      "  Accuracy:  0.8760 (Δ = +0.0143)\n",
      "  Precision: 0.8505 (Δ = +0.0021)\n",
      "  Recall:    0.9703 (Δ = +0.0247)\n",
      "  F1-Score:  0.9065 (Δ = +0.0121)\n",
      "  ROC-AUC:   0.9247 (Δ = +0.0119)\n",
      "\n",
      "✓ Tuned model is better! Updating best model.\n",
      "\n",
      "✓ Saved tuning results to xgboost_tuning_results.csv\n",
      "\n",
      "================================================================================\n",
      "SHAP Analysis - Explaining Systematic Bias Predictions\n",
      "================================================================================\n",
      "Final SHAP shape: (1960, 63)\n",
      "\n",
      "Top 15 Features Predicting Systematic Bias:\n",
      "                 feature  shap_importance\n",
      "                band_6.5         1.099160\n",
      "                band_7.0         0.402598\n",
      "        max_depth_x_band         0.393513\n",
      "                band_4.0         0.331950\n",
      "                band_5.0         0.257337\n",
      "                band_3.5         0.188454\n",
      "          lex_ttr_x_band         0.165211\n",
      "                band_5.5         0.144209\n",
      "                band_8.0         0.134466\n",
      "                 lex_ttr         0.125813\n",
      "      syn_avg_tree_depth         0.101158\n",
      "lex_avg_word_freq_x_band         0.079173\n",
      "     rst_rel_Attribution         0.072403\n",
      "          syn_noun_ratio         0.067220\n",
      "         NN_ratio_x_band         0.065280\n",
      "\n",
      "Feature Category Contributions:\n",
      "  Band/Quality: 2.707 (53.5%)\n",
      "  RST:          0.771 (15.2%)\n",
      "  Lexical:      0.173 (3.4%)\n",
      "  Syntactic:    0.357 (7.1%)\n",
      "  Interaction:  1.055 (20.8%)\n",
      "\n",
      "✓ SHAP analysis saved to systematic_bias_shap_importance.csv\n",
      "\n",
      "================================================================================\n",
      "INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "Systematic Bias Definition:\n",
      "  'A systematic difference in model performance across subgroups,\n",
      "   such that one group consistently receives less accurate or\n",
      "   systematically skewed outcomes compared to another group.'\n",
      "\n",
      "Our Findings:\n",
      "  - Identified 26 cases of systematic bias\n",
      "  - 61.9% of essays belong to disadvantaged groups\n",
      "  - Model can predict systematic bias membership with 0.907 F1-score\n",
      "  - Linguistic patterns explain 31.3% of systematic bias\n",
      "\n",
      "================================================================================\n",
      "Saving Full Results\n",
      "================================================================================\n",
      "Predicting systematic bias probability for all essays...\n",
      "✓ Predicted for 9800 essays\n",
      "✓ Full dataset saved to essays_with_systematic_bias_predictions1.csv\n",
      "\n",
      "New columns added:\n",
      "  - is_systematically_biased: Binary label (1=belongs to disadvantaged group)\n",
      "  - bias_source: Which features caused the bias label\n",
      "  - systematic_bias_probability: Model's predicted probability\n",
      "\n",
      "✓ Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Systematic Bias Detection - Group-Based Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Enhanced seed setting\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import warnings\n",
    "import re\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === Function to clean column names for XGBoost ===\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Clean column names to be XGBoost compatible.\"\"\"\n",
    "    new_columns = {}\n",
    "    for col in df.columns:\n",
    "        new_col = re.sub(r'[\\[\\]<>]', '', col)\n",
    "        new_col = new_col.replace(' ', '_').replace('(', '_').replace(')', '_').replace(',', '_')\n",
    "        new_columns[col] = new_col\n",
    "    return df.rename(columns=new_columns)\n",
    "\n",
    "# === Setup ===\n",
    "print(\"=\"*80)\n",
    "print(\"Systematic Bias Detection - Group-Based Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# === Load CSV ===\n",
    "input_csv = \"essays_with_bias_analysis.csv\"\n",
    "print(f\"Loading {input_csv}...\")\n",
    "df = pd.read_csv(input_csv)\n",
    "print(f\"✓ Loaded {len(df)} essays\")\n",
    "print()\n",
    "\n",
    "# === Parse relation_ratios ===\n",
    "print(\"=\"*80)\n",
    "print(\"Processing RST Relation Ratios\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def parse_relation_ratios(ratio_string):\n",
    "    if pd.isna(ratio_string):\n",
    "        return {}\n",
    "    result = {}\n",
    "    try:\n",
    "        pairs = str(ratio_string).split(';')\n",
    "        for pair in pairs:\n",
    "            if ':' in pair:\n",
    "                relation, ratio = pair.split(':')\n",
    "                result[relation.strip()] = float(ratio.strip())\n",
    "    except:\n",
    "        pass\n",
    "    return result\n",
    "\n",
    "relation_dicts = df['relation_ratios'].apply(parse_relation_ratios)\n",
    "all_relations = sorted(set(relation for rel_dict in relation_dicts \n",
    "                           for relation in rel_dict.keys()))\n",
    "\n",
    "print(f\"Found {len(all_relations)} unique RST relations\")\n",
    "\n",
    "for relation in all_relations:\n",
    "    df[f'rst_rel_{relation}'] = relation_dicts.apply(lambda x: x.get(relation, 0.0))\n",
    "\n",
    "relation_features = [f'rst_rel_{rel}' for rel in all_relations]\n",
    "relation_features = [f for f in relation_features if f != 'rst_rel_textual-organization']\n",
    "relation_features = [f for f in relation_features if f != 'rst_rel_Topic-Change']\n",
    "\n",
    "print(f\"Created {len(relation_features)} relation ratio features\")\n",
    "print()\n",
    "\n",
    "# === Select features ===\n",
    "rst_features = ['NN_ratio', 'NS_ratio', 'SN_ratio'] + relation_features\n",
    "lexical_features = ['lex_avg_sent_len', 'lex_ttr', 'lex_flesch_reading_ease', \n",
    "                    'lex_flesch_kincaid_grade', 'lex_avg_word_freq', 'lex_rare_word_ratio']\n",
    "syntactic_features = ['syn_noun_ratio', 'syn_verb_ratio', 'syn_adj_ratio', 'syn_adv_ratio',\n",
    "                      'syn_avg_tree_depth', 'syn_subordinate_ratio', 'syn_passive_ratio',\n",
    "                      'syn_dep_relation_diversity']\n",
    "lexical_features = [f for f in lexical_features if 'score' not in f.lower()]\n",
    "syntactic_features = [f for f in syntactic_features if 'score' not in f.lower()]\n",
    "\n",
    "all_features = rst_features + lexical_features + syntactic_features\n",
    "systematic_features = lexical_features + syntactic_features\n",
    "\n",
    "# EXPLICITLY exclude leakage\n",
    "exclude = ['discrepancy', 'abs_discrepancy', 'overall_score', 'band_numeric', \n",
    "           'is_biased', 'is_systematically_biased', 'bias_source']\n",
    "\n",
    "all_features = [f for f in all_features if f not in exclude and \n",
    "                not any(exc in f.lower() for exc in ['discrepancy', 'score', 'bias'])]\n",
    "\n",
    "print(f\"Using {len(all_features)} safe features\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Feature Selection\")\n",
    "print(\"=\"*80)\n",
    "print(f\"RST features: {len(rst_features)}\")\n",
    "print(f\"Lexical features: {len(lexical_features)}\")\n",
    "print(f\"Syntactic features: {len(syntactic_features)}\")\n",
    "print(f\"Total features: {len(all_features)}\")\n",
    "print()\n",
    "\n",
    "# # === CREATE BAND INTERACTION FEATURES ===\n",
    "print(\"=\"*80)\n",
    "print(\"Creating Band Interaction Features\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create numeric versions for grouping\n",
    "def band_to_numeric(band):\n",
    "    \"\"\"Convert band to numeric, treating '<4' as 3.5 for grouping purposes.\"\"\"\n",
    "    band = str(band).strip()\n",
    "    if band == \"<4\":\n",
    "        return 3.5\n",
    "    try:\n",
    "        return float(band)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "df['overall_score'] = pd.to_numeric(df['overall_score'], errors='coerce')\n",
    "df['band_numeric'] = df['band'].apply(band_to_numeric)\n",
    "df['band'] = df['band'].apply(band_to_numeric)\n",
    "\n",
    "# Select key features to interact with band\n",
    "key_interaction_features = [\n",
    "    #Key lexical\n",
    "    'lex_ttr', 'lex_avg_word_freq', \n",
    "    # # # Key syntactic\n",
    "    'syn_dep_relation_diversity', 'syn_passive_ratio',\n",
    "    # Key RST\n",
    "    'rst_rel_Background', 'rst_rel_Explanation', 'rst_rel_Contrast', 'rst_rel_Elaboration',\n",
    "    'NN_ratio', 'NS_ratio', 'SN_ratio', 'max_depth'\n",
    "]\n",
    "\n",
    "interaction_features = []\n",
    "\n",
    "print(\"Creating interactions for key features...\")\n",
    "for feat in key_interaction_features:\n",
    "    if feat in df.columns:\n",
    "        interaction_name = f'{feat}_x_band'\n",
    "        df[interaction_name] = df[feat] * df['band_numeric']\n",
    "        interaction_features.append(interaction_name)\n",
    "\n",
    "print(f\"✓ Created {len(interaction_features)} interaction features\")\n",
    "print(f\"  Examples: {', '.join(interaction_features[:3])}...\")\n",
    "\n",
    "all_features += interaction_features\n",
    "print(f\"Total features after interactions: {len(all_features)}\")\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def cohens_d(group1, group2):\n",
    "    \"\"\"Calculate Cohen's d effect size\"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))\n",
    "    return (np.mean(group1) - np.mean(group2)) / pooled_std if pooled_std > 0 else 0\n",
    "\n",
    "# === STEP 1: IDENTIFY SYSTEMATIC BIAS BY FEATURE ===\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: Detecting Systematic Bias Across Feature Groups\")\n",
    "print(\"=\"*80)\n",
    "print(\"Bias = Systematic difference in outcomes between subgroups\\n\")\n",
    "\n",
    "# Only analyze essays with complete features\n",
    "df_complete = df[all_features + ['discrepancy', 'band']].dropna()\n",
    "print(f\"Analyzing {len(df_complete)} essays with complete features\")\n",
    "print()\n",
    "\n",
    "# Control for quality: analyze within each band separately\n",
    "systematic_bias_results = []\n",
    "\n",
    "BIAS_MAGNITUDE_THRESHOLD = 0.09  # Minimum difference to consider \"systematic\"\n",
    "STATISTICAL_SIGNIFICANCE = 0.05  # p-value threshold\n",
    "\n",
    "for band in sorted(df_complete['band'].unique()):\n",
    "    band_data = df_complete[df_complete['band'] == band]\n",
    "    \n",
    "    if len(band_data) < 30:\n",
    "        print(f\"Skipping band {band}: insufficient data (n={len(band_data)})\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Band {band} (n={len(band_data)})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for feature in systematic_features:\n",
    "        if feature not in band_data.columns:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Create tertile groups (Low, Medium, High)\n",
    "            band_data[f'{feature}_group'] = pd.qcut(\n",
    "                band_data[feature], \n",
    "                q=3, \n",
    "                labels=['Low', 'Medium', 'High'],\n",
    "                duplicates='drop'\n",
    "            )\n",
    "            \n",
    "            # Calculate mean discrepancy per group\n",
    "            group_stats = band_data.groupby(f'{feature}_group')['discrepancy'].agg(['mean', 'std', 'count'])\n",
    "            \n",
    "            if len(group_stats) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Get means for all groups\n",
    "            low_mean = group_stats.loc['Low', 'mean'] if 'Low' in group_stats.index else None\n",
    "            medium_mean = group_stats.loc['Medium', 'mean'] if 'Medium' in group_stats.index else None\n",
    "            high_mean = group_stats.loc['High', 'mean'] if 'High' in group_stats.index else None\n",
    "            \n",
    "            # Find min and max means\n",
    "            available_means = [m for m in [low_mean, medium_mean, high_mean] if m is not None]\n",
    "            if len(available_means) < 2:\n",
    "                continue\n",
    "            \n",
    "            min_mean = min(available_means)\n",
    "            max_mean = max(available_means)\n",
    "            bias_magnitude = max_mean - min_mean\n",
    "            \n",
    "            # Identify disadvantaged (underscored) and advantaged (overscored) groups\n",
    "            disadvantaged_groups = []  # Groups with lowest mean = underscored\n",
    "            advantaged_groups = []      # Groups with highest mean = overscored\n",
    "            \n",
    "            if low_mean is not None and low_mean == min_mean:\n",
    "                disadvantaged_groups.append('Low')\n",
    "            if medium_mean is not None and medium_mean == min_mean:\n",
    "                disadvantaged_groups.append('Medium')\n",
    "            if high_mean is not None and high_mean == min_mean:\n",
    "                disadvantaged_groups.append('High')\n",
    "            \n",
    "            if low_mean is not None and low_mean == max_mean:\n",
    "                advantaged_groups.append('Low')\n",
    "            if medium_mean is not None and medium_mean == max_mean:\n",
    "                advantaged_groups.append('Medium')\n",
    "            if high_mean is not None and high_mean == max_mean:\n",
    "                advantaged_groups.append('High')\n",
    "            \n",
    "            # Statistical test: t-test between extreme groups\n",
    "            low_group = band_data[band_data[f'{feature}_group'] == 'Low']['discrepancy']\n",
    "            high_group = band_data[band_data[f'{feature}_group'] == 'High']['discrepancy']\n",
    "            \n",
    "            if len(low_group) > 1 and len(high_group) > 1:\n",
    "                t_stat, p_value = stats.ttest_ind(low_group, high_group)\n",
    "                \n",
    "                # Calculate Cohen's d effect size\n",
    "                effect_size = cohens_d(low_group, high_group)\n",
    "                \n",
    "                # Check if systematic bias exists\n",
    "                is_systematic = (abs(bias_magnitude) > BIAS_MAGNITUDE_THRESHOLD) and (p_value < STATISTICAL_SIGNIFICANCE)\n",
    "                \n",
    "                if is_systematic:\n",
    "                    # Create direction string\n",
    "                    direction_parts = []\n",
    "                    if disadvantaged_groups:\n",
    "                        direction_parts.append(f\"{', '.join(disadvantaged_groups)} {feature} → under-scored by {abs(bias_magnitude):.3f}\")\n",
    "                    if advantaged_groups:\n",
    "                        direction_parts.append(f\"{', '.join(advantaged_groups)} {feature} → over-scored by {abs(bias_magnitude):.3f}\")\n",
    "                    direction = \"; \".join(direction_parts)\n",
    "                    \n",
    "                    print(f\"  ✓ SYSTEMATIC BIAS: {feature}\")\n",
    "                    print(f\"    Magnitude: {abs(bias_magnitude):.3f} points (p={p_value:.4f}, d={effect_size:.3f})\")\n",
    "                    print(f\"    Direction: {direction}\")\n",
    "                    print(f\"    Low: {low_mean:.3f}, Medium: {medium_mean:.3f}, High: {high_mean:.3f}\")\n",
    "                    print(f\"    Disadvantaged (underscored): {', '.join(disadvantaged_groups)}\")\n",
    "                    print(f\"    Advantaged (overscored): {', '.join(advantaged_groups)}\")\n",
    "                    \n",
    "                    systematic_bias_results.append({\n",
    "                        'band': band,\n",
    "                        'feature': feature,\n",
    "                        'bias_magnitude': abs(bias_magnitude),\n",
    "                        'p_value': p_value,\n",
    "                        'cohens_d': effect_size,\n",
    "                        'low_mean': low_mean,\n",
    "                        'medium_mean': medium_mean,\n",
    "                        'high_mean': high_mean,\n",
    "                        'disadvantaged_groups': disadvantaged_groups,  # Underscored\n",
    "                        'advantaged_groups': advantaged_groups,        # Overscored\n",
    "                        'direction': direction,\n",
    "                        'n_low': len(low_group),\n",
    "                        'n_high': len(high_group)\n",
    "                    })\n",
    "        \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Systematic Bias Detection Summary\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if systematic_bias_results:\n",
    "    bias_df = pd.DataFrame(systematic_bias_results)\n",
    "    \n",
    "    # Apply FDR correction (Benjamini-Hochberg)\n",
    "    print(\"\\nApplying FDR correction for multiple comparisons...\")\n",
    "    p_values = bias_df['p_value'].values\n",
    "    reject, p_values_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n",
    "    \n",
    "    bias_df['p_value_fdr'] = p_values_corrected\n",
    "    bias_df['significant_fdr'] = reject\n",
    "    \n",
    "    print(f\"  Original significant (p < 0.05): {sum(bias_df['p_value'] < 0.05)}\")\n",
    "    print(f\"  FDR-significant (q < 0.05): {sum(reject)}\")\n",
    "    \n",
    "    bias_df = bias_df.sort_values('bias_magnitude', ascending=False)\n",
    "    \n",
    "    print(f\"\\n✓ Found {len(bias_df)} cases of systematic bias\")\n",
    "    print(\"\\nTop 10 Features with Strongest Systematic Bias:\")\n",
    "    display_cols = ['band', 'feature', 'bias_magnitude', 'cohens_d', 'p_value', 'p_value_fdr', 'direction']\n",
    "    print(bias_df[display_cols].head(10).to_string(index=False))\n",
    "    \n",
    "    # Effect size interpretation\n",
    "    small_effect = sum(abs(bias_df['cohens_d']) < 0.5)\n",
    "    medium_effect = sum((abs(bias_df['cohens_d']) >= 0.5) & (abs(bias_df['cohens_d']) < 0.8))\n",
    "    large_effect = sum(abs(bias_df['cohens_d']) >= 0.8)\n",
    "    \n",
    "    print(f\"\\nEffect Size Distribution:\")\n",
    "    print(f\"  Small (|d| < 0.5):   {small_effect} ({small_effect/len(bias_df)*100:.1f}%)\")\n",
    "    print(f\"  Medium (0.5 ≤ |d| < 0.8): {medium_effect} ({medium_effect/len(bias_df)*100:.1f}%)\")\n",
    "    print(f\"  Large (|d| ≥ 0.8):   {large_effect} ({large_effect/len(bias_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Save systematic bias findings\n",
    "    bias_df.to_csv(\"systematic_bias_findings.csv\", index=False)\n",
    "    print(f\"\\n✓ Saved detailed findings to systematic_bias_findings.csv\")\n",
    "else:\n",
    "    print(\"\\n⚠ No systematic bias detected with current thresholds\")\n",
    "    print(f\"   Try lowering BIAS_MAGNITUDE_THRESHOLD (current: {BIAS_MAGNITUDE_THRESHOLD})\")\n",
    "\n",
    "\n",
    "# === STEP 2: LABEL ESSAYS BASED ON SYSTEMATIC BIAS ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Labeling Essays by Systematic Bias Group Membership\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define: An essay is \"systematically biased\" if it belongs to a disadvantaged OR advantaged subgroup\n",
    "\n",
    "df_complete['is_systematically_biased'] = 0\n",
    "df_complete['bias_source'] = None\n",
    "df_complete['bias_direction'] = None  # Track if underscored or overscored\n",
    "\n",
    "if systematic_bias_results:\n",
    "    # For each band, identify which features show systematic bias\n",
    "    for band in df_complete['band'].unique():\n",
    "        band_biases = [b for b in systematic_bias_results if b['band'] == band]\n",
    "        \n",
    "        if not band_biases:\n",
    "            continue\n",
    "        \n",
    "        # Use top 3 most biased features per band\n",
    "        top_biases = sorted(band_biases, key=lambda x: x['bias_magnitude'], reverse=True)[:3]\n",
    "        \n",
    "        for bias_case in top_biases:\n",
    "            feature = bias_case['feature']\n",
    "            disadvantaged_groups = bias_case['disadvantaged_groups']  # Underscored\n",
    "            advantaged_groups = bias_case['advantaged_groups']        # Overscored\n",
    "            \n",
    "            # Create groups for this feature\n",
    "            try:\n",
    "                df_complete.loc[df_complete['band'] == band, f'{feature}_group'] = pd.qcut(\n",
    "                    df_complete.loc[df_complete['band'] == band, feature],\n",
    "                    q=3,\n",
    "                    labels=['Low', 'Medium', 'High'],\n",
    "                    duplicates='drop'\n",
    "                )\n",
    "                \n",
    "                # Mark essays in disadvantaged groups (underscored)\n",
    "                for disadvantaged in disadvantaged_groups:\n",
    "                    disadvantaged_mask = (df_complete['band'] == band) & \\\n",
    "                                        (df_complete[f'{feature}_group'] == disadvantaged)\n",
    "                    \n",
    "                    df_complete.loc[disadvantaged_mask, 'is_systematically_biased'] = 1\n",
    "                    \n",
    "                    # Track source and direction\n",
    "                    current_source = df_complete.loc[disadvantaged_mask, 'bias_source']\n",
    "                    df_complete.loc[disadvantaged_mask, 'bias_source'] = \\\n",
    "                        current_source.fillna('') + f\"{feature}; \"\n",
    "                    \n",
    "                    current_direction = df_complete.loc[disadvantaged_mask, 'bias_direction']\n",
    "                    df_complete.loc[disadvantaged_mask, 'bias_direction'] = \\\n",
    "                        current_direction.fillna('') + f\"{feature}:underscored; \"\n",
    "                \n",
    "                # Mark essays in advantaged groups (overscored)\n",
    "                for advantaged in advantaged_groups:\n",
    "                    advantaged_mask = (df_complete['band'] == band) & \\\n",
    "                                     (df_complete[f'{feature}_group'] == advantaged)\n",
    "                    \n",
    "                    df_complete.loc[advantaged_mask, 'is_systematically_biased'] = 1\n",
    "                    \n",
    "                    # Track source and direction\n",
    "                    current_source = df_complete.loc[advantaged_mask, 'bias_source']\n",
    "                    df_complete.loc[advantaged_mask, 'bias_source'] = \\\n",
    "                        current_source.fillna('') + f\"{feature}; \"\n",
    "                    \n",
    "                    current_direction = df_complete.loc[advantaged_mask, 'bias_direction']\n",
    "                    df_complete.loc[advantaged_mask, 'bias_direction'] = \\\n",
    "                        current_direction.fillna('') + f\"{feature}:overscored; \"\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    # Print top 3 features by band\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Top 3 Most Biased Features by Band\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    features_used_for_labeling = set()\n",
    "    for band in sorted(df_complete['band'].unique()):\n",
    "        band_biases = [b for b in systematic_bias_results if b['band'] == band]\n",
    "        \n",
    "        if not band_biases:\n",
    "            print(f\"\\nBand {band}: No systematic bias detected\")\n",
    "            continue\n",
    "        \n",
    "        top_biases = sorted(band_biases, key=lambda x: x['bias_magnitude'], reverse=True)[:3]\n",
    "        for bias_case in top_biases:\n",
    "            features_used_for_labeling.add(bias_case['feature'])\n",
    "        \n",
    "        print(f\"\\nBand {band}:\")\n",
    "        for i, bias_case in enumerate(top_biases, 1):\n",
    "            print(f\"  {i}. {bias_case['feature']}\")\n",
    "            print(f\"     Bias Magnitude:      {bias_case['bias_magnitude']:.4f}\")\n",
    "            print(f\"     Cohen's d:           {bias_case.get('cohens_d', np.nan):.4f}\")\n",
    "            print(f\"     p-value:             {bias_case['p_value']:.4f}\")\n",
    "            print(f\"     p-value (FDR):       {bias_case.get('p_value_fdr', np.nan):.4f}\")\n",
    "            print(f\"     Disadvantaged (underscored): {', '.join(bias_case['disadvantaged_groups'])}\")\n",
    "            print(f\"     Advantaged (overscored):     {', '.join(bias_case['advantaged_groups'])}\")\n",
    "\n",
    "    print(f\"\\n✓ Labeled essays based on systematic bias group membership\")\n",
    "    print(f\"  Systematically biased: {df_complete['is_systematically_biased'].sum()} \" +\n",
    "          f\"({df_complete['is_systematically_biased'].mean()*100:.1f}%)\")\n",
    "    print(f\"  Not systematically biased: {(df_complete['is_systematically_biased']==0).sum()} \" +\n",
    "          f\"({(df_complete['is_systematically_biased']==0).mean()*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n⚠ Cannot label essays: no systematic bias detected\")\n",
    "    print(\"Exiting analysis.\")\n",
    "    exit()\n",
    "\n",
    "# === STEP 3: BUILD PREDICTIVE MODEL ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: Building Predictive Model\")\n",
    "print(\"=\"*80)\n",
    "print(\"Goal: Predict which essays belong to systematically disadvantaged groups\\n\")\n",
    "\n",
    "# Exclude base features AND their interactions\n",
    "excluded_features = set(features_used_for_labeling)\n",
    "excluded_interactions = [f for f in interaction_features \n",
    "                        if any(base in f for base in excluded_features)]\n",
    "modeling_features = [f for f in all_features \n",
    "                    if f not in excluded_features and f not in excluded_interactions]\n",
    "\n",
    "complete_features = modeling_features + interaction_features\n",
    "\n",
    "print(f\"\\nFeatures for modeling: {len(modeling_features)} (down from {len(all_features)})\")\n",
    "\n",
    "X = df_complete[complete_features]\n",
    "y = df_complete['is_systematically_biased']\n",
    "\n",
    "# Add band control\n",
    "df_complete['band_categorical'] = df_complete['band'].astype(str)\n",
    "band_dummies = pd.get_dummies(df_complete['band_categorical'], prefix='band')\n",
    "X_with_band = pd.concat([X.reset_index(drop=True), band_dummies.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Clean column names\n",
    "X_with_band = clean_column_names(X_with_band)\n",
    "\n",
    "# Update feature lists\n",
    "rst_features_clean = [clean_column_names(pd.DataFrame(columns=[f])).columns[0] for f in rst_features]\n",
    "lexical_features_clean = [clean_column_names(pd.DataFrame(columns=[f])).columns[0] for f in lexical_features]\n",
    "syntactic_features_clean = [clean_column_names(pd.DataFrame(columns=[f])).columns[0] for f in syntactic_features]\n",
    "interaction_features_clean = [clean_column_names(pd.DataFrame(columns=[f])).columns[0] \n",
    "                             for f in interaction_features]\n",
    "\n",
    "print(f\"Total features: {len(X_with_band.columns)}\")\n",
    "print(f\"Target balance: {y.mean()*100:.1f}% systematically biased\")\n",
    "print()\n",
    "\n",
    "# Check if we have enough samples\n",
    "if y.sum() < 10 or (len(y) - y.sum()) < 10:\n",
    "    print(\"⚠ Insufficient samples in one or both classes\")\n",
    "    print(\"Cannot build reliable model. Try lowering BIAS_MAGNITUDE_THRESHOLD.\")\n",
    "    exit()\n",
    "\n",
    "# Split data\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_with_band, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=SEED, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} ({len(X_train)/len(X_with_band)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(X_val)} ({len(X_val)/len(X_with_band)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(X_test)} ({len(X_test)/len(X_with_band)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# === Define Models ===\n",
    "models = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(\n",
    "            n_estimators=100, max_depth=10, min_samples_split=10,\n",
    "            min_samples_leaf=5, random_state=SEED, class_weight='balanced'\n",
    "        ),\n",
    "        'scaled': False\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(\n",
    "            n_estimators=200, max_depth=8, learning_rate=0.06,\n",
    "            random_state=SEED, n_jobs=1, tree_method='hist', \n",
    "            deterministic_histogram=True, scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),\n",
    "            eval_metric='logloss'\n",
    "        ),\n",
    "        'scaled': False\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(\n",
    "            max_iter=1000, random_state=SEED, class_weight='balanced',\n",
    "            penalty='l2', C=1.0, solver='lbfgs'\n",
    "        ),\n",
    "        'scaled': True\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(\n",
    "            kernel='rbf', random_state=SEED, class_weight='balanced', probability=True\n",
    "        ),\n",
    "        'scaled': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Train and Evaluate ===\n",
    "print(\"=\"*80)\n",
    "print(\"Training Models\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model_config in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = model_config['model']\n",
    "    use_scaled = model_config['scaled']\n",
    "    \n",
    "    X_tr = X_train_scaled if use_scaled else X_train\n",
    "    X_v = X_val_scaled if use_scaled else X_val\n",
    "    X_te = X_test_scaled if use_scaled else X_test\n",
    "    \n",
    "    # Train\n",
    "    if model_name == 'XGBoost':\n",
    "        model.fit(X_tr.values, y_train.values)\n",
    "    else:\n",
    "        model.fit(X_tr, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    if model_name == 'XGBoost':\n",
    "        y_test_pred = model.predict(X_te.values)\n",
    "        proba = model.predict_proba(X_te.values)\n",
    "    else:\n",
    "        y_test_pred = model.predict(X_te)\n",
    "        proba = model.predict_proba(X_te)\n",
    "\n",
    "    # Safely extract probability for positive class\n",
    "    if proba.shape[1] == 2:\n",
    "        y_test_proba = proba[:, 1]\n",
    "    elif proba.shape[1] == 1:\n",
    "        # Only one class predicted - model is broken\n",
    "        print(f\"  ⚠ Warning: Model only predicts one class!\")\n",
    "        y_test_proba = proba[:, 0]\n",
    "    else:\n",
    "        y_test_proba = proba[:, -1]\n",
    "    \n",
    "    # Metrics\n",
    "    test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'precision': precision_score(y_test, y_test_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_test_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_test_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_test, y_test_proba) if len(np.unique(y_test)) > 1 else np.nan\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {test_metrics['f1']:.4f}\")\n",
    "    print(f\"  ROC-AUC:   {test_metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'model': model_name,\n",
    "        'test_accuracy': test_metrics['accuracy'],\n",
    "        'test_f1': test_metrics['f1'],\n",
    "        'test_roc_auc': test_metrics['roc_auc'],\n",
    "        'test_precision': test_metrics['precision'],\n",
    "        'test_recall': test_metrics['recall'],\n",
    "        'model_object': model,\n",
    "        'use_scaled': use_scaled\n",
    "    })\n",
    "\n",
    "# === Select Best Model ===\n",
    "results_df = pd.DataFrame(results)\n",
    "best_model_row = results_df.loc[results_df['test_roc_auc'].idxmax()]\n",
    "best_model_name = best_model_row['model']\n",
    "best_model = best_model_row['model_object']\n",
    "best_model_use_scaled = best_model_row['use_scaled']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"Test F1: {best_model_row['test_f1']:.4f}\")\n",
    "print(f\"Test AUC: {best_model_row['test_roc_auc']:.4f}\")\n",
    "\n",
    "# === Hyperparameter Tuning for Best Model (if XGBoost) ===\n",
    "if best_model_name == 'XGBoost':\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Hyperparameter Tuning for XGBoost (Optuna)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    import optuna\n",
    "    \n",
    "    # Suppress Optuna logging\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    \n",
    "    # Calculate scale_pos_weight for imbalanced data\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    \n",
    "    def objective_xgb(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500, step=50),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, step=0.01),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0, step=0.1),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0, step=0.1),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 0.5, step=0.1),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 1.0, step=0.1),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0, 1.0, step=0.1),\n",
    "            'random_state': SEED,\n",
    "            'scale_pos_weight': scale_pos_weight,\n",
    "            'eval_metric': 'logloss',\n",
    "            'n_jobs': 1,\n",
    "            'tree_method': 'hist',\n",
    "            'deterministic_histogram': True\n",
    "        }\n",
    "        \n",
    "        model = XGBClassifier(**params)\n",
    "        \n",
    "        # Use appropriate data format\n",
    "        X_tr = X_train if not best_model_use_scaled else X_train_scaled\n",
    "        X_v = X_val if not best_model_use_scaled else X_val_scaled\n",
    "        \n",
    "        model.fit(X_tr.values, y_train.values)\n",
    "        y_pred = model.predict(X_v.values)\n",
    "        \n",
    "        return f1_score(y_val, y_pred, zero_division=0)\n",
    "    \n",
    "    print(\"\\nStarting Optuna optimization...\")\n",
    "    print(f\"  Trials: 100\")\n",
    "    print(f\"  Objective: Maximize F1-Score on validation set\")\n",
    "    print(f\"  Current best F1: {best_model_row['test_f1']:.4f}\\n\")\n",
    "    \n",
    "    study_xgb = optuna.create_study(direction='maximize', study_name='xgb_tuning', sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "    study_xgb.optimize(objective_xgb, n_trials=100, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"\\n✓ Optimization complete!\")\n",
    "    print(f\"  Best validation F1-Score: {study_xgb.best_value:.4f}\")\n",
    "    print(f\"  Improvement: {study_xgb.best_value - best_model_row['test_f1']:.4f}\")\n",
    "    print(f\"\\nBest hyperparameters:\")\n",
    "    for param, value in study_xgb.best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    # Train final tuned model\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Training Final Tuned XGBoost Model\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    tuned_xgb = XGBClassifier(\n",
    "        **study_xgb.best_params,\n",
    "        random_state=SEED,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        eval_metric='logloss',\n",
    "        n_jobs=1,\n",
    "        tree_method='hist',\n",
    "        deterministic_histogram=True\n",
    "    )\n",
    "    \n",
    "    # Train on train+val, test on test\n",
    "    X_train_full = pd.concat([X_train, X_val])\n",
    "    y_train_full = pd.concat([y_train, y_val])\n",
    "    \n",
    "    if best_model_use_scaled:\n",
    "        X_train_full_scaled = pd.concat([X_train_scaled, X_val_scaled])\n",
    "        tuned_xgb.fit(X_train_full_scaled.values, y_train_full.values)\n",
    "        y_test_pred_tuned = tuned_xgb.predict(X_test_scaled.values)\n",
    "        y_test_proba_tuned = tuned_xgb.predict_proba(X_test_scaled.values)[:, 1]\n",
    "    else:\n",
    "        tuned_xgb.fit(X_train_full.values, y_train_full.values)\n",
    "        y_test_pred_tuned = tuned_xgb.predict(X_test.values)\n",
    "        y_test_proba_tuned = tuned_xgb.predict_proba(X_test.values)[:, 1]\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    tuned_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_test_pred_tuned),\n",
    "        'precision': precision_score(y_test, y_test_pred_tuned, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_test_pred_tuned, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_test_pred_tuned, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_test, y_test_proba_tuned)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTuned Model Performance on Test Set:\")\n",
    "    print(f\"  Accuracy:  {tuned_metrics['accuracy']:.4f} (Δ = {tuned_metrics['accuracy'] - best_model_row['test_accuracy']:+.4f})\")\n",
    "    print(f\"  Precision: {tuned_metrics['precision']:.4f} (Δ = {tuned_metrics['precision'] - best_model_row['test_precision']:+.4f})\")\n",
    "    print(f\"  Recall:    {tuned_metrics['recall']:.4f} (Δ = {tuned_metrics['recall'] - best_model_row['test_recall']:+.4f})\")\n",
    "    print(f\"  F1-Score:  {tuned_metrics['f1']:.4f} (Δ = {tuned_metrics['f1'] - best_model_row['test_f1']:+.4f})\")\n",
    "    print(f\"  ROC-AUC:   {tuned_metrics['roc_auc']:.4f} (Δ = {tuned_metrics['roc_auc'] - best_model_row['test_roc_auc']:+.4f})\")\n",
    "    \n",
    "    # Update best model if tuned version is better\n",
    "    if tuned_metrics['f1'] > best_model_row['test_f1']:\n",
    "        print(f\"\\n✓ Tuned model is better! Updating best model.\")\n",
    "        best_model = tuned_xgb\n",
    "        best_model_row = {\n",
    "            'model': 'XGBoost (Tuned)',\n",
    "            'test_accuracy': tuned_metrics['accuracy'],\n",
    "            'test_precision': tuned_metrics['precision'],\n",
    "            'test_recall': tuned_metrics['recall'],\n",
    "            'test_f1': tuned_metrics['f1'],\n",
    "            'test_roc_auc': tuned_metrics['roc_auc'],\n",
    "            'model_object': tuned_xgb,\n",
    "            'use_scaled': best_model_use_scaled\n",
    "        }\n",
    "        best_model_name = 'XGBoost'\n",
    "    else:\n",
    "        print(f\"\\n⚠ Original model performs better. Keeping original.\")\n",
    "        \n",
    "    # Save hyperparameter tuning results\n",
    "    tuning_results = pd.DataFrame({\n",
    "        'trial': range(len(study_xgb.trials)),\n",
    "        'value': [trial.value for trial in study_xgb.trials],\n",
    "        'params': [str(trial.params) for trial in study_xgb.trials]\n",
    "    })\n",
    "    tuning_results.to_csv('xgboost_tuning_results.csv', index=False)\n",
    "    print(f\"\\n✓ Saved tuning results to xgboost_tuning_results.csv\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n⚠ Best model is {best_model_name}, not XGBoost. Skipping hyperparameter tuning.\")\n",
    "\n",
    "# === SHAP Analysis ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SHAP Analysis - Explaining Systematic Bias Predictions\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_shap = X_test_scaled if best_model_use_scaled else X_test\n",
    "\n",
    "try:\n",
    "    # === 1. Choose appropriate SHAP explainer ===\n",
    "    if best_model_name in ['Random Forest', 'XGBoost']:\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "        shap_values = explainer.shap_values(X_shap.values if hasattr(X_shap, 'values') else X_shap)\n",
    "    else:\n",
    "        background = shap.sample(X_train_scaled if best_model_use_scaled else X_train, min(100, len(X_train)))\n",
    "        explainer = shap.KernelExplainer(best_model.predict_proba, background)\n",
    "        shap_values = explainer.shap_values(X_shap)\n",
    "\n",
    "    # === 2. Handle different SHAP output formats ===\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_vals = shap_values[1] if len(shap_values) > 1 else shap_values[0]\n",
    "    elif isinstance(shap_values, np.ndarray):\n",
    "        shap_vals = shap_values[:, :, 1] if shap_values.ndim == 3 else shap_values\n",
    "    else:\n",
    "        shap_vals = np.array(shap_values)\n",
    "\n",
    "    print(f\"Final SHAP shape: {shap_vals.shape}\")\n",
    "\n",
    "    # Ensure SHAP matches feature count\n",
    "    if shap_vals.shape[1] != len(X_shap.columns):\n",
    "        print(f\"⚠ Warning: SHAP shape mismatch. Trimming to {len(X_shap.columns)} features.\")\n",
    "        shap_vals = shap_vals[:, :len(X_shap.columns)]\n",
    "\n",
    "    # === 3. Calculate mean absolute SHAP importance ===\n",
    "    mean_abs_shap = np.abs(shap_vals).mean(axis=0)\n",
    "    shap_importance = (\n",
    "        pd.DataFrame({\n",
    "            'feature': list(X_shap.columns),\n",
    "            'shap_importance': mean_abs_shap\n",
    "        })\n",
    "        .sort_values('shap_importance', ascending=False)\n",
    "    )\n",
    "\n",
    "    # === 3.5. Aggregate SHAP values by feature name to remove duplicates ===\n",
    "    shap_importance = (\n",
    "        shap_importance\n",
    "        .groupby('feature', as_index=False)\n",
    "        .agg({'shap_importance': 'sum'})\n",
    "        .sort_values('shap_importance', ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"\\nTop 15 Features Predicting Systematic Bias:\")\n",
    "    print(shap_importance.head(15).to_string(index=False))\n",
    "\n",
    "    # === 4. Aggregate SHAP by feature category ===\n",
    "    rst_shap_imp = shap_importance[shap_importance['feature'].isin(rst_features_clean)]['shap_importance'].sum()\n",
    "    lex_shap_imp = shap_importance[shap_importance['feature'].isin(lexical_features_clean)]['shap_importance'].sum()\n",
    "    syn_shap_imp = shap_importance[shap_importance['feature'].isin(syntactic_features_clean)]['shap_importance'].sum()\n",
    "    band_shap_imp = shap_importance[shap_importance['feature'].str.startswith('band_')]['shap_importance'].sum()\n",
    "    interaction_shap_imp = shap_importance[shap_importance['feature'].isin(interaction_features_clean)]['shap_importance'].sum()\n",
    "\n",
    "    total_shap = rst_shap_imp + lex_shap_imp + syn_shap_imp + interaction_shap_imp + band_shap_imp\n",
    "\n",
    "    # === 5. Print category contributions ===\n",
    "    if total_shap > 0:\n",
    "        print(\"\\nFeature Category Contributions:\")\n",
    "        print(f\"  Band/Quality: {band_shap_imp:.3f} ({band_shap_imp/total_shap*100:.1f}%)\")\n",
    "        print(f\"  RST:          {rst_shap_imp:.3f} ({rst_shap_imp/total_shap*100:.1f}%)\")\n",
    "        print(f\"  Lexical:      {lex_shap_imp:.3f} ({lex_shap_imp/total_shap*100:.1f}%)\")\n",
    "        print(f\"  Syntactic:    {syn_shap_imp:.3f} ({syn_shap_imp/total_shap*100:.1f}%)\")\n",
    "        print(f\"  Interaction:  {interaction_shap_imp:.3f} ({interaction_shap_imp/total_shap*100:.1f}%)\")\n",
    "\n",
    "    # === 6. Save results ===\n",
    "    shap_importance.to_csv(\"systematic_bias_shap_importance.csv\", index=False)\n",
    "    print(\"\\n✓ SHAP analysis saved to systematic_bias_shap_importance.csv\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ SHAP analysis failed: {e}\")\n",
    "    print(\"Continuing without SHAP interpretability...\")\n",
    "    rst_shap_imp = lex_shap_imp = syn_shap_imp = band_shap_imp = total_shap = 0\n",
    "\n",
    "# === INTERPRETATION ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nSystematic Bias Definition:\")\n",
    "print(\"  'A systematic difference in model performance across subgroups,\")\n",
    "print(\"   such that one group consistently receives less accurate or\")\n",
    "print(\"   systematically skewed outcomes compared to another group.'\")\n",
    "\n",
    "print(\"\\nOur Findings:\")\n",
    "print(f\"  - Identified {len(bias_df) if systematic_bias_results else 0} cases of systematic bias\")\n",
    "print(f\"  - {df_complete['is_systematically_biased'].mean()*100:.1f}% of essays belong to disadvantaged groups\")\n",
    "print(f\"  - Model can predict systematic bias membership with {best_model_row['test_f1']:.3f} F1-score\")\n",
    "print(f\"  - Linguistic patterns explain {(interaction_shap_imp+lex_shap_imp+syn_shap_imp)/total_shap*100:.1f}% of systematic bias\")\n",
    "\n",
    "# === Save Full Results with Predictions ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Saving Full Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Add systematic bias labels back to original dataframe\n",
    "df['is_systematically_biased'] = np.nan\n",
    "df['bias_source'] = None\n",
    "\n",
    "# Merge results from df_complete back to df\n",
    "df.loc[df_complete.index, 'is_systematically_biased'] = df_complete['is_systematically_biased']\n",
    "df.loc[df_complete.index, 'bias_source'] = df_complete['bias_source']\n",
    "\n",
    "# Predict probability for all valid essays\n",
    "if 'model_object' in locals() or 'best_model' in locals():\n",
    "    print(\"Predicting systematic bias probability for all essays...\")\n",
    "    \n",
    "    # Prepare full dataset\n",
    "    df_for_prediction = df[all_features + ['band']].copy()\n",
    "    valid_pred_indices = df_for_prediction.dropna().index\n",
    "    \n",
    "    if len(valid_pred_indices) > 0:\n",
    "        X_full = df_for_prediction.loc[valid_pred_indices, all_features]\n",
    "        \n",
    "        # Add band dummies\n",
    "        band_cat_full = df.loc[valid_pred_indices, 'band'].astype(str)\n",
    "        band_dummies_full = pd.get_dummies(band_cat_full, prefix='band')\n",
    "        X_full_with_band = pd.concat([X_full.reset_index(drop=True), band_dummies_full.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "        # Clean column names\n",
    "        X_full_with_band = clean_column_names(X_full_with_band)\n",
    "        \n",
    "        # Ensure same columns as training\n",
    "        for col in X_with_band.columns:\n",
    "            if col not in X_full_with_band.columns:\n",
    "                X_full_with_band[col] = 0\n",
    "        X_full_with_band = X_full_with_band.reindex(columns=X_with_band.columns, fill_value=0)\n",
    "        \n",
    "        # Scale if needed\n",
    "        if best_model_use_scaled:\n",
    "            X_full_scaled = scaler.transform(X_full_with_band)\n",
    "            X_full_with_band = pd.DataFrame(X_full_scaled, columns=X_full_with_band.columns)\n",
    "        \n",
    "        # Predict\n",
    "        try:\n",
    "            if best_model_name == 'XGBoost':\n",
    "                proba_full = best_model.predict_proba(X_full_with_band.values)\n",
    "            else:\n",
    "                proba_full = best_model.predict_proba(X_full_with_band)\n",
    "            \n",
    "            if proba_full.shape[1] == 2:\n",
    "                bias_proba_full = proba_full[:, 1]\n",
    "            else:\n",
    "                bias_proba_full = proba_full[:, 0]\n",
    "            \n",
    "            # Add to dataframe\n",
    "            df['systematic_bias_probability'] = np.nan\n",
    "            df.loc[valid_pred_indices, 'systematic_bias_probability'] = bias_proba_full\n",
    "            \n",
    "            print(f\"✓ Predicted for {len(valid_pred_indices)} essays\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Could not predict probabilities: {e}\")\n",
    "            df['systematic_bias_probability'] = np.nan\n",
    "\n",
    "output_full = \"essays_with_systematic_bias_predictions1.csv\"\n",
    "df.to_csv(output_full, index=False)\n",
    "print(f\"✓ Full dataset saved to {output_full}\")\n",
    "print(f\"\\nNew columns added:\")\n",
    "print(f\"  - is_systematically_biased: Binary label (1=belongs to disadvantaged group)\")\n",
    "print(f\"  - bias_source: Which features caused the bias label\")\n",
    "print(f\"  - systematic_bias_probability: Model's predicted probability\")\n",
    "\n",
    "print(\"\\n✓ Analysis complete!\")\n",
    "\n",
    "# # After training best model:\n",
    "# import pickle\n",
    "# pickle.dump(best_model, open('best_model.pkl_xg', 'wb'))\n",
    "# pickle.dump(scaler, open('scaler.pkl_xg', 'wb'))\n",
    "# pickle.dump(list(X_with_band.columns), open('training_columns_xg.pkl', 'wb'))\n",
    "# pickle.dump(best_model_name, open('best_model_name_xg.pkl', 'wb'))\n",
    "# pickle.dump(best_model_use_scaled, open('best_model_use_scaled_xg.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9d67a",
   "metadata": {},
   "source": [
    "### 2.3 Systematic Bias and Discrepancy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d78f2cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SYSTEMATIC BIAS AND DISCREPANCY OVERLAP ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Loading data...\n",
      "✓ Loaded 9800 essays\n",
      "\n",
      "Essays with valid data: 9800\n",
      "\n",
      "================================================================================\n",
      "RESULTS BY BAND\n",
      "================================================================================\n",
      "\n",
      "Band 3.5:\n",
      "  N Discrepancy              = 383\n",
      "  N Systematically Biased    = 0\n",
      "  Both (Biased & Discrepancy) = 0\n",
      "  Biased Only (no discrepancy) = 0\n",
      "  Discrepancy Only (not biased) = 383\n",
      "\n",
      "Band 4.0:\n",
      "  N Discrepancy              = 108\n",
      "  N Systematically Biased    = 0\n",
      "  Both (Biased & Discrepancy) = 0\n",
      "  Biased Only (no discrepancy) = 0\n",
      "  Discrepancy Only (not biased) = 108\n",
      "\n",
      "Band 4.5:\n",
      "  N Discrepancy              = 105\n",
      "  N Systematically Biased    = 431\n",
      "  Both (Biased & Discrepancy) = 68\n",
      "  Biased Only (no discrepancy) = 363\n",
      "  Discrepancy Only (not biased) = 37\n",
      "\n",
      "Band 5.0:\n",
      "  N Discrepancy              = 554\n",
      "  N Systematically Biased    = 969\n",
      "  Both (Biased & Discrepancy) = 512\n",
      "  Biased Only (no discrepancy) = 457\n",
      "  Discrepancy Only (not biased) = 42\n",
      "\n",
      "Band 5.5:\n",
      "  N Discrepancy              = 227\n",
      "  N Systematically Biased    = 839\n",
      "  Both (Biased & Discrepancy) = 196\n",
      "  Biased Only (no discrepancy) = 643\n",
      "  Discrepancy Only (not biased) = 31\n",
      "\n",
      "Band 6.0:\n",
      "  N Discrepancy              = 217\n",
      "  N Systematically Biased    = 761\n",
      "  Both (Biased & Discrepancy) = 152\n",
      "  Biased Only (no discrepancy) = 609\n",
      "  Discrepancy Only (not biased) = 65\n",
      "\n",
      "Band 6.5:\n",
      "  N Discrepancy              = 118\n",
      "  N Systematically Biased    = 0\n",
      "  Both (Biased & Discrepancy) = 0\n",
      "  Biased Only (no discrepancy) = 0\n",
      "  Discrepancy Only (not biased) = 118\n",
      "\n",
      "Band 7.0:\n",
      "  N Discrepancy              = 306\n",
      "  N Systematically Biased    = 1300\n",
      "  Both (Biased & Discrepancy) = 295\n",
      "  Biased Only (no discrepancy) = 1005\n",
      "  Discrepancy Only (not biased) = 11\n",
      "\n",
      "Band 7.5:\n",
      "  N Discrepancy              = 129\n",
      "  N Systematically Biased    = 737\n",
      "  Both (Biased & Discrepancy) = 89\n",
      "  Biased Only (no discrepancy) = 648\n",
      "  Discrepancy Only (not biased) = 40\n",
      "\n",
      "Band 8.0:\n",
      "  N Discrepancy              = 277\n",
      "  N Systematically Biased    = 643\n",
      "  Both (Biased & Discrepancy) = 254\n",
      "  Biased Only (no discrepancy) = 389\n",
      "  Discrepancy Only (not biased) = 23\n",
      "\n",
      "Band 8.5:\n",
      "  N Discrepancy              = 33\n",
      "  N Systematically Biased    = 317\n",
      "  Both (Biased & Discrepancy) = 25\n",
      "  Biased Only (no discrepancy) = 292\n",
      "  Discrepancy Only (not biased) = 8\n",
      "\n",
      "Band 9.0:\n",
      "  N Discrepancy              = 79\n",
      "  N Systematically Biased    = 72\n",
      "  Both (Biased & Discrepancy) = 52\n",
      "  Biased Only (no discrepancy) = 20\n",
      "  Discrepancy Only (not biased) = 27\n",
      "\n",
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      "✓ Saved: systematic_bias_overlap_by_band.csv\n",
      "\n",
      "================================================================================\n",
      "OVERALL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total Essays with Discrepancy:     2536\n",
      "Total Systematically Biased:       6069\n",
      "  Both (Biased & Discrepancy):     1643\n",
      "  Biased Only (no discrepancy):    4426\n",
      "  Discrepancy Only (not biased):   893\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Systematic Bias and Discrepancy Analysis\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SYSTEMATIC BIAS AND DISCREPANCY OVERLAP ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load data\n",
    "print(\"\\nLoading data...\")\n",
    "df = pd.read_csv('essays_with_systematic_bias_predictions1.csv')\n",
    "print(f\"✓ Loaded {len(df)} essays\\n\")\n",
    "\n",
    "# Ensure numeric types\n",
    "df['overall_score'] = pd.to_numeric(df['overall_score'], errors='coerce')\n",
    "df['discrepancy'] = pd.to_numeric(df['discrepancy'], errors='coerce')\n",
    "\n",
    "# Remove missing values\n",
    "df_clean = df[['band', 'overall_score', 'discrepancy', 'is_systematically_biased']].dropna()\n",
    "print(f\"Essays with valid data: {len(df_clean)}\\n\")\n",
    "\n",
    "# Analysis by band\n",
    "print(\"=\"*80)\n",
    "print(\"RESULTS BY BAND\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for band in sorted(df_clean['band'].unique()):\n",
    "    band_data = df_clean[df_clean['band'] == band]\n",
    "    \n",
    "    # Create boolean masks\n",
    "    has_discrepancy = band_data['discrepancy'] != 0\n",
    "    is_sys_biased = band_data['is_systematically_biased'] == 1\n",
    "    \n",
    "    # Calculate overlap categories\n",
    "    both = (has_discrepancy & is_sys_biased).sum()\n",
    "    biased_only = (is_sys_biased & ~has_discrepancy).sum()\n",
    "    discrepancy_only = (has_discrepancy & ~is_sys_biased).sum()\n",
    "    \n",
    "    # Totals\n",
    "    n_discrepancy = has_discrepancy.sum()\n",
    "    n_sys_biased = is_sys_biased.sum()\n",
    "    \n",
    "    results.append({\n",
    "        'Band': band,\n",
    "        'N_Discrepancy': n_discrepancy,\n",
    "        'N_Sys_Biased': n_sys_biased,\n",
    "        'Both': both,\n",
    "        'Biased_Only': biased_only,\n",
    "        'Discrepancy_Only': discrepancy_only\n",
    "    })\n",
    "    \n",
    "    # Print in requested format\n",
    "    print(f\"\\nBand {band}:\")\n",
    "    print(f\"  N Discrepancy              = {n_discrepancy}\")\n",
    "    print(f\"  N Systematically Biased    = {n_sys_biased}\")\n",
    "    print(f\"  Both (Biased & Discrepancy) = {both}\")\n",
    "    print(f\"  Biased Only (no discrepancy) = {biased_only}\")\n",
    "    print(f\"  Discrepancy Only (not biased) = {discrepancy_only}\")\n",
    "\n",
    "# Save results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('systematic_bias_overlap_by_band.csv', index=False)\n",
    "print(\"✓ Saved: systematic_bias_overlap_by_band.csv\")\n",
    "\n",
    "# Overall summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "has_discrepancy_all = df_clean['discrepancy'] != 0\n",
    "is_sys_biased_all = df_clean['is_systematically_biased'] == 1\n",
    "\n",
    "both_all = (has_discrepancy_all & is_sys_biased_all).sum()\n",
    "biased_only_all = (is_sys_biased_all & ~has_discrepancy_all).sum()\n",
    "discrepancy_only_all = (has_discrepancy_all & ~is_sys_biased_all).sum()\n",
    "\n",
    "total_discrepancy = has_discrepancy_all.sum()\n",
    "total_sys_biased = is_sys_biased_all.sum()\n",
    "\n",
    "print(f\"\\nTotal Essays with Discrepancy:     {total_discrepancy}\")\n",
    "print(f\"Total Systematically Biased:       {total_sys_biased}\")\n",
    "print(f\"  Both (Biased & Discrepancy):     {both_all}\")\n",
    "print(f\"  Biased Only (no discrepancy):    {biased_only_all}\")\n",
    "print(f\"  Discrepancy Only (not biased):   {discrepancy_only_all}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c418da07",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572a2ab",
   "metadata": {},
   "source": [
    "### 3.1 Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69d71a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Running Cross-Validation on Training Set\n",
      "================================================================================\n",
      "\n",
      "Fold 1\n",
      "\n",
      "  Model: Random Forest\n",
      "    Accuracy:  0.8495\n",
      "    Precision: 0.8149\n",
      "    Recall:    0.9794\n",
      "    F1-Score:  0.8896\n",
      "    ROC-AUC:   0.9069\n",
      "\n",
      "  Model: XGBoost\n",
      "    Accuracy:  0.8452\n",
      "    Precision: 0.8362\n",
      "    Recall:    0.9327\n",
      "    F1-Score:  0.8818\n",
      "    ROC-AUC:   0.9074\n",
      "\n",
      "  Model: Logistic Regression\n",
      "    Accuracy:  0.8214\n",
      "    Precision: 0.8174\n",
      "    Recall:    0.9162\n",
      "    F1-Score:  0.8640\n",
      "    ROC-AUC:   0.8931\n",
      "\n",
      "  Model: SVM\n",
      "    Accuracy:  0.8452\n",
      "    Precision: 0.8197\n",
      "    Recall:    0.9615\n",
      "    F1-Score:  0.8850\n",
      "    ROC-AUC:   0.8974\n",
      "\n",
      "Fold 2\n",
      "\n",
      "  Model: Random Forest\n",
      "    Accuracy:  0.8469\n",
      "    Precision: 0.8093\n",
      "    Recall:    0.9849\n",
      "    F1-Score:  0.8885\n",
      "    ROC-AUC:   0.8980\n",
      "\n",
      "  Model: XGBoost\n",
      "    Accuracy:  0.8529\n",
      "    Precision: 0.8331\n",
      "    Recall:    0.9533\n",
      "    F1-Score:  0.8892\n",
      "    ROC-AUC:   0.9046\n",
      "\n",
      "  Model: Logistic Regression\n",
      "    Accuracy:  0.8163\n",
      "    Precision: 0.8114\n",
      "    Recall:    0.9162\n",
      "    F1-Score:  0.8606\n",
      "    ROC-AUC:   0.8841\n",
      "\n",
      "  Model: SVM\n",
      "    Accuracy:  0.8359\n",
      "    Precision: 0.8071\n",
      "    Recall:    0.9657\n",
      "    F1-Score:  0.8793\n",
      "    ROC-AUC:   0.8789\n",
      "\n",
      "Fold 3\n",
      "\n",
      "  Model: Random Forest\n",
      "    Accuracy:  0.8444\n",
      "    Precision: 0.8052\n",
      "    Recall:    0.9876\n",
      "    F1-Score:  0.8871\n",
      "    ROC-AUC:   0.8878\n",
      "\n",
      "  Model: XGBoost\n",
      "    Accuracy:  0.8605\n",
      "    Precision: 0.8365\n",
      "    Recall:    0.9629\n",
      "    F1-Score:  0.8953\n",
      "    ROC-AUC:   0.9072\n",
      "\n",
      "  Model: Logistic Regression\n",
      "    Accuracy:  0.8138\n",
      "    Precision: 0.8100\n",
      "    Recall:    0.9135\n",
      "    F1-Score:  0.8586\n",
      "    ROC-AUC:   0.8768\n",
      "\n",
      "  Model: SVM\n",
      "    Accuracy:  0.8384\n",
      "    Precision: 0.8106\n",
      "    Recall:    0.9643\n",
      "    F1-Score:  0.8808\n",
      "    ROC-AUC:   0.8715\n",
      "\n",
      "Fold 4\n",
      "\n",
      "  Model: Random Forest\n",
      "    Accuracy:  0.8529\n",
      "    Precision: 0.8223\n",
      "    Recall:    0.9725\n",
      "    F1-Score:  0.8911\n",
      "    ROC-AUC:   0.9136\n",
      "\n",
      "  Model: XGBoost\n",
      "    Accuracy:  0.8554\n",
      "    Precision: 0.8402\n",
      "    Recall:    0.9464\n",
      "    F1-Score:  0.8902\n",
      "    ROC-AUC:   0.9136\n",
      "\n",
      "  Model: Logistic Regression\n",
      "    Accuracy:  0.8214\n",
      "    Precision: 0.8166\n",
      "    Recall:    0.9176\n",
      "    F1-Score:  0.8642\n",
      "    ROC-AUC:   0.8985\n",
      "\n",
      "  Model: SVM\n",
      "    Accuracy:  0.8563\n",
      "    Precision: 0.8269\n",
      "    Recall:    0.9712\n",
      "    F1-Score:  0.8932\n",
      "    ROC-AUC:   0.8945\n",
      "\n",
      "Fold 5\n",
      "\n",
      "  Model: Random Forest\n",
      "    Accuracy:  0.8537\n",
      "    Precision: 0.8147\n",
      "    Recall:    0.9890\n",
      "    F1-Score:  0.8934\n",
      "    ROC-AUC:   0.9086\n",
      "\n",
      "  Model: XGBoost\n",
      "    Accuracy:  0.8554\n",
      "    Precision: 0.8455\n",
      "    Recall:    0.9383\n",
      "    F1-Score:  0.8895\n",
      "    ROC-AUC:   0.9045\n",
      "\n",
      "  Model: Logistic Regression\n",
      "    Accuracy:  0.8291\n",
      "    Precision: 0.8284\n",
      "    Recall:    0.9136\n",
      "    F1-Score:  0.8689\n",
      "    ROC-AUC:   0.8980\n",
      "\n",
      "  Model: SVM\n",
      "    Accuracy:  0.8512\n",
      "    Precision: 0.8267\n",
      "    Recall:    0.9616\n",
      "    F1-Score:  0.8890\n",
      "    ROC-AUC:   0.8894\n",
      "\n",
      "Cross-Validation Summary (Mean ± Std by Model):\n",
      "                    accuracy         precision          recall          \\\n",
      "                        mean     std      mean     std    mean     std   \n",
      "model                                                                    \n",
      "Logistic Regression   0.8204  0.0059    0.8168  0.0072  0.9154  0.0018   \n",
      "Random Forest         0.8495  0.0039    0.8133  0.0065  0.9827  0.0068   \n",
      "SVM                   0.8454  0.0085    0.8182  0.0091  0.9648  0.0039   \n",
      "XGBoost               0.8539  0.0056    0.8383  0.0047  0.9467  0.0120   \n",
      "\n",
      "                         f1         roc_auc          \n",
      "                       mean     std    mean     std  \n",
      "model                                                \n",
      "Logistic Regression  0.8633  0.0039  0.8901  0.0094  \n",
      "Random Forest        0.8899  0.0024  0.9030  0.0102  \n",
      "SVM                  0.8855  0.0058  0.8864  0.0109  \n",
      "XGBoost              0.8892  0.0048  0.9075  0.0037  \n"
     ]
    }
   ],
   "source": [
    "# CROSS-VALIDATION\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Use the same training data you used in your pipeline\n",
    "X_cv = X_train.copy()\n",
    "y_cv = y_train.copy()\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "cv_results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Running Cross-Validation on Training Set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_cv, y_cv)):\n",
    "    print(f\"\\nFold {fold+1}\")\n",
    "    \n",
    "    X_tr = X_cv.iloc[train_idx]\n",
    "    X_val = X_cv.iloc[val_idx]\n",
    "    y_tr = y_cv.iloc[train_idx]\n",
    "    y_val = y_cv.iloc[val_idx]\n",
    "    \n",
    "    for model_name, model_config in models.items():\n",
    "        print(f\"\\n  Model: {model_name}\")\n",
    "        \n",
    "        model = model_config['model']\n",
    "        use_scaled = model_config['scaled']\n",
    "        \n",
    "        # Scale if needed\n",
    "        if use_scaled:\n",
    "            scaler_cv = StandardScaler()\n",
    "            X_tr_scaled = pd.DataFrame(scaler_cv.fit_transform(X_tr), columns=X_tr.columns, index=X_tr.index)\n",
    "            X_val_scaled = pd.DataFrame(scaler_cv.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
    "            X_tr_final, X_val_final = X_tr_scaled, X_val_scaled\n",
    "        else:\n",
    "            X_tr_final, X_val_final = X_tr, X_val\n",
    "        \n",
    "        # Train\n",
    "        if model_name == 'XGBoost':\n",
    "            model.fit(X_tr_final.values, y_tr.values)\n",
    "            y_pred = model.predict(X_val_final.values)\n",
    "            proba = model.predict_proba(X_val_final.values)\n",
    "        else:\n",
    "            model.fit(X_tr_final, y_tr)\n",
    "            y_pred = model.predict(X_val_final)\n",
    "            proba = model.predict_proba(X_val_final)\n",
    "        \n",
    "        # Extract probability for positive class\n",
    "        y_proba = proba[:, 1] if proba.shape[1] == 2 else proba[:, 0]\n",
    "        \n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            'fold': fold + 1,\n",
    "            'model': model_name,\n",
    "            'accuracy': accuracy_score(y_val, y_pred),\n",
    "            'precision': precision_score(y_val, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_val, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_val, y_pred, zero_division=0),\n",
    "            'roc_auc': roc_auc_score(y_val, y_proba) if len(np.unique(y_val)) > 1 else np.nan\n",
    "        }\n",
    "        \n",
    "        print(f\"    Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"    Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"    Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"    F1-Score:  {metrics['f1']:.4f}\")\n",
    "        print(f\"    ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "        \n",
    "        cv_results.append(metrics)\n",
    "\n",
    "# Convert to DataFrame\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nCross-Validation Summary (Mean ± Std by Model):\")\n",
    "summary = cv_df.groupby('model').agg(['mean', 'std']).round(4)\n",
    "print(summary[['accuracy', 'precision', 'recall', 'f1', 'roc_auc']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ce7ab",
   "metadata": {},
   "source": [
    "### 3.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b97ea42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: XGBoost\n",
      "Test F1-Score: 0.9065\n",
      "Test Accuracy: 0.8760\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted\n",
      "                Not Biased    Biased\n",
      "Actual     No     539            207\n",
      "           Yes     36           1178\n",
      "\n",
      "Detailed Breakdown:\n",
      "  True Negatives (TN):    539 ( 27.5%) - Correctly identified not biased\n",
      "  True Positives (TP):   1178 ( 60.1%) - Correctly identified biased\n",
      "  False Positives (FP):   207 ( 10.6%) - Incorrectly flagged as biased (Type I error)\n",
      "  False Negatives (FN):    36 (  1.8%) - Missed bias (Type II error)\n",
      "\n",
      "Performance Metrics:\n",
      "  Total Correct:  1717 ( 87.6%)\n",
      "  Total Errors:    243 ( 12.4%)\n",
      "\n",
      "  Accuracy:  0.8760\n",
      "  Precision: 0.8505 (of essays flagged as biased, 85.1% were correct)\n",
      "  Recall:    0.9703 (caught 97.0% of biased essays)\n",
      "  F1-Score:  0.9065\n",
      "\n",
      "Error Analysis:\n",
      "  → More False Positives than False Negatives (207 vs 36)\n",
      "  → Model tends to OVER-PREDICT bias (too sensitive)\n",
      "  → Consider increasing decision threshold\n",
      "\n",
      "✓ Saved: confusion_matrix_best_model.png\n",
      "✓ Saved: confusion_matrix_breakdown.csv\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# === Confusion Matrix for Best Model ===\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get best model's predictions (should already exist from your training code)\n",
    "# Make sure you're using the test set predictions from the BEST model\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Test F1-Score: {best_model_row['test_f1']:.4f}\")\n",
    "print(f\"Test Accuracy: {best_model_row['test_accuracy']:.4f}\")\n",
    "\n",
    "# Get predictions from best model\n",
    "if best_model_use_scaled:\n",
    "    X_test_final = X_test_scaled\n",
    "else:\n",
    "    X_test_final = X_test\n",
    "\n",
    "if best_model_name == 'XGBoost':\n",
    "    y_pred_best = best_model.predict(X_test_final.values)\n",
    "    y_proba_best = best_model.predict_proba(X_test_final.values)[:, 1]\n",
    "else:\n",
    "    y_pred_best = best_model.predict(X_test_final)\n",
    "    y_proba_best = best_model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "# Extract values\n",
    "tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
    "total = cm.sum()\n",
    "\n",
    "# Print detailed breakdown\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"{'':>15} Predicted\")\n",
    "print(f\"{'':>15} Not Biased    Biased\")\n",
    "print(f\"{'Actual':<10} No  {tn:>6}         {fp:>6}\")\n",
    "print(f\"{'':>10} Yes {fn:>6}         {tp:>6}\")\n",
    "\n",
    "print(f\"\\nDetailed Breakdown:\")\n",
    "print(f\"  True Negatives (TN):   {tn:4d} ({tn/total*100:5.1f}%) - Correctly identified not biased\")\n",
    "print(f\"  True Positives (TP):   {tp:4d} ({tp/total*100:5.1f}%) - Correctly identified biased\")\n",
    "print(f\"  False Positives (FP):  {fp:4d} ({fp/total*100:5.1f}%) - Incorrectly flagged as biased (Type I error)\")\n",
    "print(f\"  False Negatives (FN):  {fn:4d} ({fn/total*100:5.1f}%) - Missed bias (Type II error)\")\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Total Correct:  {tn+tp:4d} ({(tn+tp)/total*100:5.1f}%)\")\n",
    "print(f\"  Total Errors:   {fp+fn:4d} ({(fp+fn)/total*100:5.1f}%)\")\n",
    "\n",
    "# Calculate metrics manually to verify\n",
    "accuracy = (tp + tn) / total\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\n  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f} (of essays flagged as biased, {precision*100:.1f}% were correct)\")\n",
    "print(f\"  Recall:    {recall:.4f} (caught {recall*100:.1f}% of biased essays)\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "# Error analysis\n",
    "print(f\"\\nError Analysis:\")\n",
    "if fp > fn:\n",
    "    print(f\"  → More False Positives than False Negatives ({fp} vs {fn})\")\n",
    "    print(f\"  → Model tends to OVER-PREDICT bias (too sensitive)\")\n",
    "    print(f\"  → Consider increasing decision threshold\")\n",
    "elif fn > fp:\n",
    "    print(f\"  → More False Negatives than False Positives ({fn} vs {fp})\")\n",
    "    print(f\"  → Model tends to MISS bias (too conservative)\")\n",
    "    print(f\"  → Consider decreasing decision threshold\")\n",
    "else:\n",
    "    print(f\"  → Balanced error distribution ({fp} FP, {fn} FN)\")\n",
    "    print(f\"  → Good precision-recall trade-off\")\n",
    "\n",
    "# === Visualization 1: Standard Confusion Matrix ===\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Count-based\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "            xticklabels=['Not Biased', 'Biased'],\n",
    "            yticklabels=['Not Biased', 'Biased'],\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            annot_kws={'size': 14})\n",
    "ax1.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Confusion Matrix - {best_model_name}\\n(Counts)', \n",
    "              fontsize=13, fontweight='bold')\n",
    "\n",
    "# Plot 2: Percentage-based\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues', ax=ax2,\n",
    "            xticklabels=['Not Biased', 'Biased'],\n",
    "            yticklabels=['Not Biased', 'Biased'],\n",
    "            cbar_kws={'label': 'Percentage (%)'},\n",
    "            annot_kws={'size': 14})\n",
    "ax2.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Confusion Matrix - {best_model_name}\\n(Percentages)', \n",
    "              fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'Model Performance: F1={f1:.3f}, Accuracy={accuracy:.3f}', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_best_model.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\n✓ Saved: confusion_matrix_best_model.png\")\n",
    "\n",
    "# === Save confusion matrix data ===\n",
    "cm_data = pd.DataFrame({\n",
    "    'Metric': ['True Negatives (TN)', 'False Positives (FP)', \n",
    "               'False Negatives (FN)', 'True Positives (TP)',\n",
    "               'Total Correct', 'Total Errors', 'Accuracy', 'Error Rate'],\n",
    "    'Count': [tn, fp, fn, tp, tn+tp, fp+fn, tn+tp, fp+fn],\n",
    "    'Percentage': [f\"{tn/total*100:.1f}%\", f\"{fp/total*100:.1f}%\",\n",
    "                   f\"{fn/total*100:.1f}%\", f\"{tp/total*100:.1f}%\",\n",
    "                   f\"{(tn+tp)/total*100:.1f}%\", f\"{(fp+fn)/total*100:.1f}%\",\n",
    "                   f\"{accuracy*100:.1f}%\", f\"{(fp+fn)/total*100:.1f}%\"],\n",
    "    'Description': [\n",
    "        'Correctly predicted not biased',\n",
    "        'Incorrectly predicted as biased (Type I error)',\n",
    "        'Missed bias - predicted as not biased (Type II error)',\n",
    "        'Correctly predicted biased',\n",
    "        'All correct predictions',\n",
    "        'All incorrect predictions',\n",
    "        'Overall accuracy',\n",
    "        'Overall error rate'\n",
    "    ]\n",
    "})\n",
    "\n",
    "cm_data.to_csv('confusion_matrix_breakdown.csv', index=False)\n",
    "print(\"✓ Saved: confusion_matrix_breakdown.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6141e48c",
   "metadata": {},
   "source": [
    "### 3.3 Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d512d0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Probability Calibration\n",
      "================================================================================\n",
      "\n",
      "Calibration Curve Analysis:\n",
      "Predicted Prob       Actual Rate          Difference     \n",
      "-------------------------------------------------------\n",
      "0.00                 0.00                 0.00           \n",
      "0.17                 0.00                 0.17           \n",
      "0.24                 0.04                 0.20           \n",
      "0.35                 0.30                 0.05           \n",
      "0.45                 0.48                 0.03           \n",
      "0.56                 0.70                 0.15           \n",
      "0.65                 0.65                 0.01           \n",
      "0.75                 0.73                 0.03           \n",
      "0.86                 0.83                 0.03           \n",
      "0.95                 0.96                 0.01           \n",
      "\n",
      "================================================================================\n",
      "CALIBRATION METRICS\n",
      "================================================================================\n",
      "\n",
      "📊 Expected Calibration Error (ECE):\n",
      "   0.0229 - Average calibration error (weighted by bin size)\n",
      "\n",
      "📊 Maximum Calibration Error (MCE):\n",
      "   0.1957 - Worst-case calibration error across all bins\n",
      "\n",
      "📊 Adaptive Calibration Error (ACE):\n",
      "   0.0190 - ECE with adaptive (quantile-based) binning\n",
      "\n",
      "================================================================================\n",
      "INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "ECE (Expected Calibration Error):\n",
      "  ✅ Excellent (0.0229) - Well calibrated\n",
      "\n",
      "MCE (Maximum Calibration Error):\n",
      "  ❌ Poor (0.1957) - Poorly calibrated - significant over/under-confidence\n",
      "\n",
      "ACE (Adaptive Calibration Error):\n",
      "  ✅ Excellent (0.0190) - Well calibrated\n",
      "\n",
      "================================================================================\n",
      "DETAILED BIN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "📋 Uniform Bins (for ECE/MCE):\n",
      "Bin Range                 Confidence   Accuracy     Error      Samples   \n",
      "--------------------------------------------------------------------------------\n",
      "[0.00, 0.10]              0.0037       0.0000       0.0037     461       \n",
      "[0.10, 0.20]              0.1667       0.0000       0.1667     8         \n",
      "[0.20, 0.30]              0.2392       0.0435       0.1957     23        \n",
      "[0.30, 0.40]              0.3473       0.2963       0.0510     27        \n",
      "[0.40, 0.50]              0.4514       0.4821       0.0308     56        \n",
      "[0.50, 0.60]              0.5567       0.7019       0.1452     104       \n",
      "[0.60, 0.70]              0.6539       0.6479       0.0060     142       \n",
      "[0.70, 0.80]              0.7524       0.7268       0.0256     183       \n",
      "[0.80, 0.90]              0.8583       0.8300       0.0283     300       \n",
      "[0.90, 1.00]              0.9542       0.9619       0.0077     656       \n",
      "\n",
      "📋 Adaptive Bins (for ACE):\n",
      "Bin Range                 Confidence   Accuracy     Error      Samples   \n",
      "--------------------------------------------------------------------------------\n",
      "[0.00, 0.00]              0.0018       0.0000       0.0018     196       \n",
      "[0.00, 0.00]              0.0032       0.0000       0.0032     196       \n",
      "[0.00, 0.52]              0.2489       0.2245       0.0244     196       \n",
      "[0.52, 0.68]              0.6054       0.6837       0.0783     196       \n",
      "[0.68, 0.79]              0.7354       0.6990       0.0364     196       \n",
      "[0.79, 0.87]              0.8317       0.8214       0.0103     196       \n",
      "[0.87, 0.92]              0.8943       0.8827       0.0117     196       \n",
      "[0.92, 0.95]              0.9349       0.9490       0.0141     196       \n",
      "[0.95, 0.97]              0.9626       0.9592       0.0034     196       \n",
      "[0.97, 0.99]              0.9810       0.9745       0.0065     196       \n",
      "\n",
      "✓ Saved: calibration_analysis.png\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATION\n",
      "================================================================================\n",
      "\n",
      "⚠️ Your model has CALIBRATION ISSUES\n",
      "   - Predicted probabilities may not reflect true confidence\n",
      "   - Strongly recommend applying calibration methods:\n",
      "     • Platt Scaling (Logistic Calibration)\n",
      "     • Isotonic Regression\n",
      "     • Temperature Scaling\n",
      "   - Example: CalibratedClassifierCV from sklearn\n"
     ]
    }
   ],
   "source": [
    "# === Calibration Analysis ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Probability Calibration\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np\n",
    "\n",
    "# Calculate calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_proba_best, n_bins=10)\n",
    "\n",
    "print(f\"\\nCalibration Curve Analysis:\")\n",
    "print(f\"{'Predicted Prob':<20} {'Actual Rate':<20} {'Difference':<15}\")\n",
    "print(\"-\"*55)\n",
    "for pred, true in zip(prob_pred, prob_true):\n",
    "    diff = abs(true - pred)\n",
    "    print(f\"{pred:<20.2f} {true:<20.2f} {diff:<15.2f}\")\n",
    "\n",
    "# === Calibration Metrics ===\n",
    "def calculate_calibration_metrics(y_true, y_prob, n_bins=10, strategy='uniform'):\n",
    "    \"\"\"\n",
    "    Calculate ECE, MCE, and ACE calibration metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True binary labels (0 or 1)\n",
    "    y_prob : array-like\n",
    "        Predicted probabilities\n",
    "    n_bins : int\n",
    "        Number of bins for uniform binning (ECE, MCE)\n",
    "    strategy : str\n",
    "        'uniform' for ECE/MCE, 'quantile' for ACE\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "    \n",
    "    if strategy == 'uniform':\n",
    "        # Uniform binning for ECE and MCE\n",
    "        bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "        bin_indices = np.digitize(y_prob, bin_edges[1:-1])\n",
    "    else:\n",
    "        # Adaptive (quantile) binning for ACE\n",
    "        bin_edges = np.percentile(y_prob, np.linspace(0, 100, n_bins + 1))\n",
    "        bin_edges = np.unique(bin_edges)  # Remove duplicates\n",
    "        if len(bin_edges) <= 2:\n",
    "            # Fallback if not enough unique values\n",
    "            bin_edges = np.linspace(y_prob.min(), y_prob.max(), n_bins + 1)\n",
    "        bin_indices = np.digitize(y_prob, bin_edges[1:-1])\n",
    "    \n",
    "    ece = 0.0\n",
    "    mce = 0.0\n",
    "    bin_details = []\n",
    "    \n",
    "    for i in range(len(bin_edges) - 1):\n",
    "        bin_mask = (bin_indices == i)\n",
    "        \n",
    "        if np.sum(bin_mask) > 0:\n",
    "            bin_acc = np.mean(y_true[bin_mask])  # Actual accuracy in bin\n",
    "            bin_conf = np.mean(y_prob[bin_mask])  # Average confidence in bin\n",
    "            bin_size = np.sum(bin_mask) / len(y_true)  # Fraction of samples\n",
    "            \n",
    "            calibration_error = np.abs(bin_acc - bin_conf)\n",
    "            \n",
    "            # ECE: weighted average\n",
    "            ece += bin_size * calibration_error\n",
    "            \n",
    "            # MCE: maximum error\n",
    "            mce = max(mce, calibration_error)\n",
    "            \n",
    "            bin_details.append({\n",
    "                'bin_range': (bin_edges[i], bin_edges[i+1]),\n",
    "                'accuracy': bin_acc,\n",
    "                'confidence': bin_conf,\n",
    "                'error': calibration_error,\n",
    "                'size': bin_size,\n",
    "                'count': np.sum(bin_mask)\n",
    "            })\n",
    "    \n",
    "    return ece, mce, bin_details\n",
    "\n",
    "# Calculate ECE and MCE (uniform binning)\n",
    "ece, mce, bin_details_uniform = calculate_calibration_metrics(\n",
    "    y_test, y_proba_best, n_bins=10, strategy='uniform'\n",
    ")\n",
    "\n",
    "# Calculate ACE (adaptive/quantile binning)\n",
    "ace, ace_max, bin_details_adaptive = calculate_calibration_metrics(\n",
    "    y_test, y_proba_best, n_bins=10, strategy='quantile'\n",
    ")\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALIBRATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📊 Expected Calibration Error (ECE):\")\n",
    "print(f\"   {ece:.4f} - Average calibration error (weighted by bin size)\")\n",
    "\n",
    "print(\"\\n📊 Maximum Calibration Error (MCE):\")\n",
    "print(f\"   {mce:.4f} - Worst-case calibration error across all bins\")\n",
    "\n",
    "print(\"\\n📊 Adaptive Calibration Error (ACE):\")\n",
    "print(f\"   {ace:.4f} - ECE with adaptive (quantile-based) binning\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def interpret_calibration(metric_value, metric_name):\n",
    "    \"\"\"Provide interpretation for calibration metrics\"\"\"\n",
    "    if metric_value < 0.05:\n",
    "        level = \"Excellent\"\n",
    "        emoji = \"✅\"\n",
    "        desc = \"Well calibrated\"\n",
    "    elif metric_value < 0.10:\n",
    "        level = \"Good\"\n",
    "        emoji = \"✓\"\n",
    "        desc = \"Reasonably calibrated\"\n",
    "    elif metric_value < 0.15:\n",
    "        level = \"Moderate\"\n",
    "        emoji = \"⚠️\"\n",
    "        desc = \"Moderate calibration issues\"\n",
    "    else:\n",
    "        level = \"Poor\"\n",
    "        emoji = \"❌\"\n",
    "        desc = \"Poorly calibrated - significant over/under-confidence\"\n",
    "    \n",
    "    print(f\"\\n{metric_name}:\")\n",
    "    print(f\"  {emoji} {level} ({metric_value:.4f}) - {desc}\")\n",
    "\n",
    "interpret_calibration(ece, \"ECE (Expected Calibration Error)\")\n",
    "interpret_calibration(mce, \"MCE (Maximum Calibration Error)\")\n",
    "interpret_calibration(ace, \"ACE (Adaptive Calibration Error)\")\n",
    "\n",
    "# Detailed bin analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED BIN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📋 Uniform Bins (for ECE/MCE):\")\n",
    "print(f\"{'Bin Range':<25} {'Confidence':<12} {'Accuracy':<12} {'Error':<10} {'Samples':<10}\")\n",
    "print(\"-\"*80)\n",
    "for detail in bin_details_uniform:\n",
    "    bin_range = f\"[{detail['bin_range'][0]:.2f}, {detail['bin_range'][1]:.2f}]\"\n",
    "    print(f\"{bin_range:<25} {detail['confidence']:<12.4f} {detail['accuracy']:<12.4f} \"\n",
    "          f\"{detail['error']:<10.4f} {detail['count']:<10}\")\n",
    "\n",
    "print(\"\\n📋 Adaptive Bins (for ACE):\")\n",
    "print(f\"{'Bin Range':<25} {'Confidence':<12} {'Accuracy':<12} {'Error':<10} {'Samples':<10}\")\n",
    "print(\"-\"*80)\n",
    "for detail in bin_details_adaptive:\n",
    "    bin_range = f\"[{detail['bin_range'][0]:.2f}, {detail['bin_range'][1]:.2f}]\"\n",
    "    print(f\"{bin_range:<25} {detail['confidence']:<12.4f} {detail['accuracy']:<12.4f} \"\n",
    "          f\"{detail['error']:<10.4f} {detail['count']:<10}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1,2,figsize=(16, 6))\n",
    "\n",
    "# Left plot: Calibration curve\n",
    "axes[0].plot(prob_pred, prob_true, marker='o', linewidth=2, markersize=8, label='Model')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Perfect Calibration')\n",
    "axes[0].set_xlabel('Mean Predicted Probability', fontsize=12)\n",
    "axes[0].set_ylabel('Fraction of Positives', fontsize=12)\n",
    "axes[0].set_title('Calibration Curve', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Right plot: Calibration metrics comparison\n",
    "metrics = ['ECE', 'MCE', 'ACE']\n",
    "values = [ece, mce, ace]\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "\n",
    "bars = axes[1].bar(metrics, values, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "axes[1].axhline(y=0.05, color='green', linestyle='--', linewidth=1, label='Excellent (<0.05)')\n",
    "axes[1].axhline(y=0.10, color='orange', linestyle='--', linewidth=1, label='Good (<0.10)')\n",
    "axes[1].axhline(y=0.15, color='red', linestyle='--', linewidth=1, label='Moderate (<0.15)')\n",
    "axes[1].set_ylabel('Calibration Error', fontsize=12)\n",
    "axes[1].set_title('Calibration Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='upper right', fontsize=9)\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{value:.4f}',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('calibration_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\n✓ Saved: calibration_analysis.png\")\n",
    "\n",
    "# Summary recommendation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "avg_calibration = (ece + ace) / 2  # Average ECE and ACE\n",
    "if avg_calibration < 0.05 and mce < 0.10:\n",
    "    print(\"\\n✅ Your model is WELL CALIBRATED!\")\n",
    "    print(\"   - Predicted probabilities closely match true outcome rates\")\n",
    "    print(\"   - Safe to use probabilities for decision-making\")\n",
    "elif avg_calibration < 0.10 and mce < 0.15:\n",
    "    print(\"\\n✓ Your model is REASONABLY CALIBRATED\")\n",
    "    print(\"   - Predicted probabilities are generally reliable\")\n",
    "    print(\"   - Consider calibration methods (e.g., Platt scaling) for improvement\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Your model has CALIBRATION ISSUES\")\n",
    "    print(\"   - Predicted probabilities may not reflect true confidence\")\n",
    "    print(\"   - Strongly recommend applying calibration methods:\")\n",
    "    print(\"     • Platt Scaling (Logistic Calibration)\")\n",
    "    print(\"     • Isotonic Regression\")\n",
    "    print(\"     • Temperature Scaling\")\n",
    "    print(\"   - Example: CalibratedClassifierCV from sklearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f61bdc",
   "metadata": {},
   "source": [
    "### 3.4 External Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2e5cba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXTERNAL VALIDATION - Applying Model to New Dataset\n",
      "================================================================================\n",
      "\n",
      "Loaded external dataset: 9487 essays\n",
      "Creating interactions for key features...\n",
      "✓ Created 12 interaction features\n",
      "  Examples: lex_ttr_x_band, lex_avg_word_freq_x_band, syn_dep_relation_diversity_x_band...\n",
      "================================================================================\n",
      "Processing RST Relation Ratios\n",
      "================================================================================\n",
      "Complete cases: 9487 essays\n",
      "\n",
      "Processing Band 3.5...\n",
      "\n",
      "Processing Band 4.0...\n",
      "\n",
      "Processing Band 4.5...\n",
      "  ✓ lex_rare_word_ratio: 401 essays labeled (underscored: Low; overscored: High)\n",
      "\n",
      "Processing Band 5.0...\n",
      "  ✓ syn_subordinate_ratio: 685 essays labeled (underscored: High; overscored: Low)\n",
      "  ✓ lex_flesch_kincaid_grade: 686 essays labeled (underscored: Low; overscored: High)\n",
      "  ✓ lex_flesch_reading_ease: 682 essays labeled (underscored: High; overscored: Low)\n",
      "\n",
      "Processing Band 5.5...\n",
      "  ✓ lex_flesch_kincaid_grade: 620 essays labeled (underscored: Low; overscored: High)\n",
      "  ✓ lex_flesch_reading_ease: 623 essays labeled (underscored: High; overscored: Low)\n",
      "  ✓ lex_avg_sent_len: 622 essays labeled (underscored: Low; overscored: High)\n",
      "\n",
      "Processing Band 6.0...\n",
      "  ✓ lex_flesch_reading_ease: 744 essays labeled (underscored: High; overscored: Low)\n",
      "\n",
      "Processing Band 6.5...\n",
      "\n",
      "Processing Band 7.0...\n",
      "  ✓ lex_flesch_kincaid_grade: 895 essays labeled (underscored: Low; overscored: High)\n",
      "  ✓ syn_adj_ratio: 889 essays labeled (underscored: Low; overscored: High)\n",
      "  ✓ lex_rare_word_ratio: 895 essays labeled (underscored: Low; overscored: High)\n",
      "  ✓ lex_flesch_reading_ease: 896 essays labeled (underscored: High; overscored: Low)\n",
      "\n",
      "Processing Band 7.5...\n",
      "  ✓ lex_rare_word_ratio: 745 essays labeled (underscored: Low; overscored: High)\n",
      "\n",
      "Processing Band 8.0...\n",
      "  ✓ lex_flesch_kincaid_grade: 469 essays labeled (underscored: Low; overscored: High)\n",
      "  ✓ syn_adj_ratio: 464 essays labeled (underscored: Low; overscored: High)\n",
      "  ✓ lex_flesch_reading_ease: 470 essays labeled (underscored: High; overscored: Low)\n",
      "\n",
      "Processing Band 8.5...\n",
      "  ✓ lex_flesch_kincaid_grade: 287 essays labeled (underscored: Low; overscored: Medium)\n",
      "  ✓ lex_flesch_reading_ease: 286 essays labeled (underscored: High; overscored: Medium)\n",
      "\n",
      "Processing Band 9.0...\n",
      "  ✓ lex_flesch_reading_ease: 51 essays labeled (underscored: Medium; overscored: Low)\n",
      "\n",
      "✓ External dataset labeled\n",
      "  Systematically biased: 5983 (63.1%)\n",
      "  Not systematically biased: 3504 (36.9%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Preparing Features for Prediction\n",
      "--------------------------------------------------------------------------------\n",
      "Modeling features: 39\n",
      "Target distribution: 63.1% biased\n",
      "\n",
      "Aligning features with training data...\n",
      "  Training features: 51\n",
      "  External features before alignment: 51\n",
      "  External features before alignment: 63\n",
      "  ✓ Features used without scaling\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Applying Trained Model\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Predictions complete for 9487 essays\n",
      "\n",
      "================================================================================\n",
      "EXTERNAL VALIDATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "External Dataset Performance:\n",
      "  Accuracy:     0.9526\n",
      "  Precision:    0.9467\n",
      "  Recall:       0.9799\n",
      "  F1-Score:     0.9630\n",
      "  ROC-AUC:      0.9810\n",
      "  PR-AUC:       0.9851\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted\n",
      "              Not Biased  Biased\n",
      "  Actual  No      3174        330\n",
      "          Yes      120       5863\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "   Dataset Bias Rate Accuracy F1-Score ROC-AUC\n",
      "  Internal     61.9%   0.8760   0.9065  0.9247\n",
      "  External     63.1%   0.9526   0.9630  0.9810\n",
      "Difference    +1.1pp  +0.0765  +0.0565 +0.0562\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Generalization Assessment\n",
      "--------------------------------------------------------------------------------\n",
      "  ✓ GOOD: F1-score shows acceptable variation\n",
      "    → Reasonable generalization (Δ = +0.0565)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Saving Results\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Saved external predictions to: essays_with_features_2_with_predictions.csv\n",
      "✓ Saved comparison to: internal_vs_external_comparison1.csv\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# EXTERNAL VALIDATION\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTERNAL VALIDATION - Applying Model to New Dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load external dataset\n",
    "df_external = pd.read_csv('essays_with_features_filtered.csv')\n",
    "print(f\"\\nLoaded external dataset: {len(df_external)} essays\")\n",
    "\n",
    "def band_to_numeric(band):\n",
    "    \"\"\"Convert band to numeric, treating '<4' as 3.5 for grouping purposes.\"\"\"\n",
    "    band = str(band).strip()\n",
    "    if band == \"<4\":\n",
    "        return 3.5\n",
    "    try:\n",
    "        return float(band)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "df_external['overall_score'] = pd.to_numeric(df_external['overall_score'], errors='coerce')\n",
    "df_external['band_numeric'] = df_external['band'].apply(band_to_numeric)\n",
    "df_external['band'] = df_external['band'].apply(band_to_numeric)\n",
    "\n",
    "# Select key features to interact with band\n",
    "key_interaction_features = [\n",
    "    # Key lexical\n",
    "    'lex_ttr', 'lex_avg_word_freq', \n",
    "    # Key syntactic\n",
    "    'syn_dep_relation_diversity', 'syn_passive_ratio',\n",
    "    # Key RST\n",
    "    'rst_rel_Background', 'rst_rel_Explanation', 'rst_rel_Contrast', 'rst_rel_Elaboration',\n",
    "    'NN_ratio', 'NS_ratio', 'SN_ratio', 'max_depth'\n",
    "]\n",
    "\n",
    "interaction_features = []\n",
    "\n",
    "print(\"Creating interactions for key features...\")\n",
    "for feat in key_interaction_features:\n",
    "    if feat in df_external.columns:\n",
    "        interaction_name = f'{feat}_x_band'\n",
    "        df_external[interaction_name] = df_external[feat] * df_external['band_numeric']\n",
    "        interaction_features.append(interaction_name)\n",
    "\n",
    "print(f\"✓ Created {len(interaction_features)} interaction features\")\n",
    "print(f\"  Examples: {', '.join(interaction_features[:3])}...\")\n",
    "\n",
    "# === Function to clean column names for XGBoost ===\n",
    "def clean_column_names(df_external):\n",
    "    \"\"\"Clean column names to be XGBoost compatible.\"\"\"\n",
    "    new_columns = {}\n",
    "    for col in df_external.columns:\n",
    "        col_str = str(col)\n",
    "        new_col = re.sub(r'[\\[\\]<>]', '', col_str)\n",
    "        new_col = new_col.replace(' ', '_').replace('(', '_').replace(')', '_').replace(',', '_')\n",
    "        new_columns[col] = new_col\n",
    "    return df_external.rename(columns=new_columns)\n",
    "\n",
    "df_external = clean_column_names(df_external)\n",
    "# === Parse relation_ratios ===\n",
    "print(\"=\"*80)\n",
    "print(\"Processing RST Relation Ratios\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if required features exist\n",
    "missing_features = [f for f in features_used_for_labeling if f not in df_external.columns]\n",
    "if missing_features:\n",
    "    print(f\"⚠ Warning: Missing features in external data: {missing_features}\")\n",
    "    print(\"These features must be calculated first!\")\n",
    "    # You may need to extract features if they're not in the CSV\n",
    "\n",
    "# Keep only complete cases\n",
    "df_external_complete = df_external[list(features_used_for_labeling) + ['band']].dropna()\n",
    "print(f\"Complete cases: {len(df_external_complete)} essays\")\n",
    "\n",
    "\n",
    "df_external_complete['is_systematically_biased'] = 0\n",
    "df_external_complete['bias_source'] = None\n",
    "df_external_complete['bias_direction'] = None  # Track underscored vs overscored\n",
    "\n",
    "# Apply same labeling logic as internal dataset\n",
    "df_external_complete['bias_direction'] = None  # Track underscored vs overscored\n",
    "\n",
    "for band in sorted(df_external_complete['band'].unique()):\n",
    "    print(f\"\\nProcessing Band {band}...\")\n",
    "    band_mask = df_external_complete['band'] == band\n",
    "    \n",
    "    for feature in features_used_for_labeling:\n",
    "        if feature not in df_external_complete.columns:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Create tertile groups (same as internal dataset)\n",
    "            df_external_complete.loc[band_mask, f'{feature}_group'] = pd.qcut(\n",
    "                df_external_complete.loc[band_mask, feature],\n",
    "                q=3,\n",
    "                labels=['Low', 'Medium', 'High'],\n",
    "                duplicates='drop'\n",
    "            )\n",
    "\n",
    "            # Find the disadvantaged and advantaged groups from internal analysis\n",
    "            disadvantaged_groups = None\n",
    "            advantaged_groups = None\n",
    "            for bias_case in systematic_bias_results:\n",
    "                if bias_case['band'] == band and bias_case['feature'] == feature:\n",
    "                    disadvantaged_groups = bias_case['disadvantaged_groups']  # Underscored\n",
    "                    advantaged_groups = bias_case['advantaged_groups']        # Overscored\n",
    "                    break\n",
    "            \n",
    "            if disadvantaged_groups is None and advantaged_groups is None:\n",
    "                # If this feature wasn't biased in this band, skip\n",
    "                continue\n",
    "            \n",
    "            # Mark essays in disadvantaged groups (underscored)\n",
    "            total_underscored = 0\n",
    "            if disadvantaged_groups:\n",
    "                for disadvantaged_group in disadvantaged_groups:\n",
    "                    disadvantaged_mask = (df_external_complete['band'] == band) & \\\n",
    "                                        (df_external_complete[f'{feature}_group'] == disadvantaged_group)\n",
    "                    \n",
    "                    df_external_complete.loc[disadvantaged_mask, 'is_systematically_biased'] = 1\n",
    "                    \n",
    "                    # Track source and direction\n",
    "                    current_source = df_external_complete.loc[disadvantaged_mask, 'bias_source']\n",
    "                    df_external_complete.loc[disadvantaged_mask, 'bias_source'] = \\\n",
    "                        current_source.fillna('') + f\"{feature}; \"\n",
    "                    \n",
    "                    current_direction = df_external_complete.loc[disadvantaged_mask, 'bias_direction']\n",
    "                    df_external_complete.loc[disadvantaged_mask, 'bias_direction'] = \\\n",
    "                        current_direction.fillna('') + f\"{feature}:underscored; \"\n",
    "                    \n",
    "                    total_underscored += disadvantaged_mask.sum()\n",
    "            \n",
    "            # Mark essays in advantaged groups (overscored)\n",
    "            total_overscored = 0\n",
    "            if advantaged_groups:\n",
    "                for advantaged_group in advantaged_groups:\n",
    "                    advantaged_mask = (df_external_complete['band'] == band) & \\\n",
    "                                     (df_external_complete[f'{feature}_group'] == advantaged_group)\n",
    "                    \n",
    "                    df_external_complete.loc[advantaged_mask, 'is_systematically_biased'] = 1\n",
    "                    \n",
    "                    # Track source and direction\n",
    "                    current_source = df_external_complete.loc[advantaged_mask, 'bias_source']\n",
    "                    df_external_complete.loc[advantaged_mask, 'bias_source'] = \\\n",
    "                        current_source.fillna('') + f\"{feature}; \"\n",
    "                    \n",
    "                    current_direction = df_external_complete.loc[advantaged_mask, 'bias_direction']\n",
    "                    df_external_complete.loc[advantaged_mask, 'bias_direction'] = \\\n",
    "                        current_direction.fillna('') + f\"{feature}:overscored; \"\n",
    "                    \n",
    "                    total_overscored += advantaged_mask.sum()\n",
    "            \n",
    "            # Report both\n",
    "            groups_str = []\n",
    "            if disadvantaged_groups:\n",
    "                groups_str.append(f\"underscored: {', '.join(disadvantaged_groups)}\")\n",
    "            if advantaged_groups:\n",
    "                groups_str.append(f\"overscored: {', '.join(advantaged_groups)}\")\n",
    "            \n",
    "            print(f\"  ✓ {feature}: {total_underscored + total_overscored} essays labeled ({'; '.join(groups_str)})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ {feature}: Could not process - {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\n✓ External dataset labeled\")\n",
    "print(f\"  Systematically biased: {df_external_complete['is_systematically_biased'].sum()} \" +\n",
    "      f\"({df_external_complete['is_systematically_biased'].mean()*100:.1f}%)\")\n",
    "print(f\"  Not systematically biased: {(df_external_complete['is_systematically_biased']==0).sum()} \" +\n",
    "      f\"({(df_external_complete['is_systematically_biased']==0).mean()*100:.1f}%)\")\n",
    "\n",
    "# === Prepare Features for Model ===\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Preparing Features for Prediction\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Get modeling features (excluding labeling features)\n",
    "X_external = df_external[complete_features]\n",
    "y_external = df_external_complete['is_systematically_biased']\n",
    "\n",
    "print(f\"Modeling features: {len(modeling_features)}\")\n",
    "print(f\"Target distribution: {y_external.mean()*100:.1f}% biased\")\n",
    "\n",
    "# # Add band dummies\n",
    "df_external_complete['band_categorical'] = df_external_complete['band'].astype(str)\n",
    "band_dummies_ext = pd.get_dummies(df_external_complete['band_categorical'], prefix='band')\n",
    "X_external = pd.DataFrame(X_external)\n",
    "\n",
    "X_external_with_band = pd.concat([X_external.reset_index(drop=True), \n",
    "                                   band_dummies_ext.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Clean column names\n",
    "X_external_with_band.columns = X_external_with_band.columns.astype(str)\n",
    "X_external_with_band = clean_column_names(X_external_with_band)\n",
    "\n",
    "# Ensure same columns as training data\n",
    "print(f\"\\nAligning features with training data...\")\n",
    "print(f\"  Training features: {len(complete_features)}\")\n",
    "print(f\"  External features before alignment: {len(X_external.columns)}\")\n",
    "print(f\"  External features before alignment: {len(X_external_with_band.columns)}\")\n",
    "\n",
    "\n",
    "# Scale using the SAME scaler from training (don't refit!)\n",
    "if best_model_use_scaled:\n",
    "    X_external_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_external_with_band),\n",
    "        columns=X_external_with_band.columns,\n",
    "        index=X_external_with_band.index\n",
    "    )\n",
    "    X_external_final = X_external_scaled\n",
    "    print(\"  ✓ Features scaled using training scaler\")\n",
    "else:\n",
    "    X_external_final = X_external_with_band\n",
    "    print(\"  ✓ Features used without scaling\")\n",
    "\n",
    "# === Apply Trained Model ===\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Applying Trained Model\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Predict using the trained model\n",
    "if best_model_name == 'XGBoost':\n",
    "    y_pred_external = best_model.predict(X_external_final.values)\n",
    "    y_proba_external_raw = best_model.predict_proba(X_external_final.values)\n",
    "else:\n",
    "    y_pred_external = best_model.predict(X_external_final)\n",
    "    y_proba_external_raw = best_model.predict_proba(X_external_final)\n",
    "\n",
    "# Extract probability for positive class\n",
    "if y_proba_external_raw.shape[1] == 2:\n",
    "    y_proba_external = y_proba_external_raw[:, 1]\n",
    "else:\n",
    "    y_proba_external = y_proba_external_raw[:, 0]\n",
    "\n",
    "print(f\"✓ Predictions complete for {len(y_pred_external)} essays\")\n",
    "\n",
    "# === Evaluate External Performance ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTERNAL VALIDATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix,\n",
    "                             classification_report, average_precision_score)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_ext = accuracy_score(y_external, y_pred_external)\n",
    "precision_ext = precision_score(y_external, y_pred_external, zero_division=0)\n",
    "recall_ext = recall_score(y_external, y_pred_external, zero_division=0)\n",
    "f1_ext = f1_score(y_external, y_pred_external, zero_division=0)\n",
    "roc_auc_ext = roc_auc_score(y_external, y_proba_external)\n",
    "pr_auc_ext = average_precision_score(y_external, y_proba_external)\n",
    "\n",
    "print(f\"\\nExternal Dataset Performance:\")\n",
    "print(f\"  Accuracy:     {accuracy_ext:.4f}\")\n",
    "print(f\"  Precision:    {precision_ext:.4f}\")\n",
    "print(f\"  Recall:       {recall_ext:.4f}\")\n",
    "print(f\"  F1-Score:     {f1_ext:.4f}\")\n",
    "print(f\"  ROC-AUC:      {roc_auc_ext:.4f}\")\n",
    "print(f\"  PR-AUC:       {pr_auc_ext:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_ext = confusion_matrix(y_external, y_pred_external)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"              Not Biased  Biased\")\n",
    "print(f\"  Actual  No    {cm_ext[0,0]:6d}     {cm_ext[0,1]:6d}\")\n",
    "print(f\"          Yes   {cm_ext[1,0]:6d}     {cm_ext[1,1]:6d}\")\n",
    "\n",
    "# === Compare Internal vs External ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Dataset': ['Internal', 'External', 'Difference'],\n",
    "    'Bias Rate': [\n",
    "        f\"{y_train.mean():.1%}\",\n",
    "        f\"{y_external.mean():.1%}\",\n",
    "        f\"{(y_external.mean() - y_train.mean())*100:+.1f}pp\"\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        f\"{best_model_row['test_accuracy']:.4f}\",\n",
    "        f\"{accuracy_ext:.4f}\",\n",
    "        f\"{accuracy_ext - best_model_row['test_accuracy']:+.4f}\"\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f\"{best_model_row['test_f1']:.4f}\",\n",
    "        f\"{f1_ext:.4f}\",\n",
    "        f\"{f1_ext - best_model_row['test_f1']:+.4f}\"\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        f\"{best_model_row['test_roc_auc']:.4f}\",\n",
    "        f\"{roc_auc_ext:.4f}\",\n",
    "        f\"{roc_auc_ext - best_model_row['test_roc_auc']:+.4f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Generalization Assessment\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "f1_diff = f1_ext - best_model_row['test_f1']\n",
    "auc_diff = roc_auc_ext - best_model_row['test_roc_auc']\n",
    "\n",
    "if abs(f1_diff) < 0.05:\n",
    "    print(\"  ✓ EXCELLENT: F1-score is consistent across datasets\")\n",
    "    print(f\"    → Model generalizes well (Δ = {f1_diff:+.4f})\")\n",
    "elif abs(f1_diff) < 0.10:\n",
    "    print(\"  ✓ GOOD: F1-score shows acceptable variation\")\n",
    "    print(f\"    → Reasonable generalization (Δ = {f1_diff:+.4f})\")\n",
    "else:\n",
    "    print(\"  ⚠ MODERATE: Notable F1-score difference\")\n",
    "    print(f\"    → Some generalization challenges (Δ = {f1_diff:+.4f})\")\n",
    "\n",
    "if abs(auc_diff) > 0.10:\n",
    "    print(f\"\\n  Note: ROC-AUC difference is substantial (Δ = {auc_diff:+.4f})\")\n",
    "    print(f\"    → Likely due to class imbalance difference\")\n",
    "    print(f\"    → Internal bias rate: {y_train.mean():.1%}\")\n",
    "    print(f\"    → External bias rate: {y_external.mean():.1%}\")\n",
    "\n",
    "# === Save Results ===\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Saving Results\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Add predictions to external dataframe\n",
    "df_external_complete['predicted_bias'] = y_pred_external\n",
    "df_external_complete['bias_probability'] = y_proba_external\n",
    "df_external_complete['prediction_correct'] = (y_pred_external == y_external).astype(int)\n",
    "df_external_complete['essay'] = df_external['essay']\n",
    "df_external_complete['overall_score'] = df_external['overall_score']\n",
    "\n",
    "# Save\n",
    "output_file = 'essays_with_features_2_with_predictions.csv'\n",
    "df_external_complete.to_csv(output_file, index=False)\n",
    "print(f\"✓ Saved external predictions to: {output_file}\")\n",
    "\n",
    "# Save comparison metrics\n",
    "comparison_df.to_csv('internal_vs_external_comparison.csv', index=False)\n",
    "print(f\"✓ Saved comparison to: internal_vs_external_comparison1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817484b",
   "metadata": {},
   "source": [
    "### 4. OTHER VISUALIZATIONS AND OUTPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934bf0dc",
   "metadata": {},
   "source": [
    "### 4.1 External Dataset Band Distribution Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d68446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Dataset Band Distribution Table\n",
    "import pandas as pd\n",
    "\n",
    "# === Load dataset ===\n",
    "df = pd.read_csv(\"essays_with_features_2.csv\")\n",
    "\n",
    "# Ensure 'band' is treated as string or categorical\n",
    "df['band'] = df['band'].astype(str)\n",
    "\n",
    "# === Count essays per band ===\n",
    "band_counts = (\n",
    "    df['band']\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    ")\n",
    "band_counts.columns = ['Band Score', 'Number of Essays']\n",
    "\n",
    "# === Add percentage of total ===\n",
    "band_counts['Percentage (%)'] = (\n",
    "    band_counts['Number of Essays'] / band_counts['Number of Essays'].sum() * 100\n",
    ").round(2)\n",
    "\n",
    "# === Print to console ===\n",
    "print(\"\\n=== Number of Essays per Band Score ===\")\n",
    "print(band_counts.to_string(index=False))\n",
    "\n",
    "# === Export to LaTeX ===\n",
    "latex_table = band_counts.to_latex(\n",
    "    index=False,\n",
    "    caption=\"Distribution of Essays by Band Score\",\n",
    "    label=\"tab:band_distribution\",\n",
    "    float_format=\"%.2f\",\n",
    "    column_format=\"ccc\",  # center-align all columns\n",
    "    bold_rows=True\n",
    ")\n",
    "\n",
    "# Save LaTeX table to file (optional)\n",
    "with open(\"band_distribution_table.tex\", \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"\\nLaTeX table saved as 'band_distribution_table.tex'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e9eb23",
   "metadata": {},
   "source": [
    "### 4.2 Bias Magnitude Across Bands by Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias Magnitude Across Bands by Feature Visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data from your LaTeX table\n",
    "data = [\n",
    "    [\"lex_rare_word_ratio\", 4.5, 0.1582],\n",
    "    [\"lex_flesch_kincaid_grade\", 5.0, 0.3129],\n",
    "    [\"lex_flesch_reading_ease\", 5.0, 0.2837],\n",
    "    #[\"syn_subordinate_ratio\", 5.0, 0.2323],\n",
    "    #[\"syn_avg_tree_depth\", 5.0, 0.1956],\n",
    "    #[\"syn_verb_ratio\", 5.0, 0.1683],\n",
    "    [\"lex_flesch_kincaid_grade\", 5.5, 0.1562],\n",
    "    [\"lex_flesch_reading_ease\", 5.5, 0.1405],\n",
    "    [\"lex_avg_sent_len\", 5.5, 0.1206],\n",
    "    #[\"syn_avg_tree_depth\", 5.5, 0.1131],\n",
    "    #[\"syn_verb_ratio\", 5.5, 0.104],\n",
    "    [\"lex_flesch_reading_ease\", 6.0, 0.0971],\n",
    "    #[\"syn_adj_ratio\", 7.0, 0.1498],\n",
    "    [\"lex_rare_word_ratio\", 7.0, 0.1492],\n",
    "    [\"lex_flesch_reading_ease\", 7.0, 0.1296],\n",
    "    [\"lex_flesch_kincaid_grade\", 7.0, 0.1201],\n",
    "    #[\"syn_noun_ratio\", 7.0, 0.1024],\n",
    "    #[\"syn_noun_ratio\", 7.5, 0.18],\n",
    "    [\"lex_flesch_reading_ease\", 8.0, 0.2076],\n",
    "    #[\"syn_adj_ratio\", 8.0, 0.1896],\n",
    "    [\"lex_flesch_kincaid_grade\", 8.0, 0.156],\n",
    "    [\"lex_flesch_reading_ease\", 8.5, 0.115],\n",
    "    [\"lex_flesch_kincaid_grade\", 8.5, 0.1099],\n",
    "    [\"lex_flesch_reading_ease\", 9.0, 0.5]\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Feature\", \"Band\", \"Bias Magnitude\"])\n",
    "\n",
    "# Pivot for plotting (features as separate lines)\n",
    "pivot_df = df.pivot_table(index=\"Band\", columns=\"Feature\", values=\"Bias Magnitude\")\n",
    "\n",
    "# Sort bands numerically\n",
    "pivot_df = pivot_df.sort_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for feature in pivot_df.columns:\n",
    "    plt.plot(pivot_df.index, pivot_df[feature], marker='o', label=feature)\n",
    "\n",
    "# Labels & formatting\n",
    "plt.title(\"Bias Magnitude Across Bands by Feature\", fontsize=14, weight='bold')\n",
    "plt.xlabel(\"Band Score\", fontsize=12)\n",
    "plt.ylabel(\"Bias Magnitude\", fontsize=12)\n",
    "plt.legend(title=\"Feature\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show or save the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29747e8",
   "metadata": {},
   "source": [
    "### 4.3 Cohen's d Effect Size by Band and Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fc2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohen's d Effect Size by Band and Feature Visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Data from your LaTeX table ---\n",
    "data = [\n",
    "    [\"lex_rare_word_ratio\", 4.5, -0.2269, \"Low\"],\n",
    "    [\"lex_flesch_kincaid_grade\", 5.0, -0.3017, \"Low\"],\n",
    "    [\"lex_flesch_reading_ease\", 5.0, 0.2689, \"High\"],\n",
    "    [\"syn_subordinate_ratio\", 5.0, 0.2244, \"High\"],\n",
    "    [\"syn_avg_tree_depth\", 5.0, -0.1878, \"Low\"],\n",
    "    [\"syn_verb_ratio\", 5.0, 0.1674, \"High\"],\n",
    "    [\"lex_flesch_kincaid_grade\", 5.5, -0.3234, \"Low\"],\n",
    "    [\"lex_flesch_reading_ease\", 5.5, 0.2978, \"High\"],\n",
    "    [\"lex_avg_sent_len\", 5.5, -0.2576, \"Low\"],\n",
    "    [\"syn_avg_tree_depth\", 5.5, -0.2255, \"Low\"],\n",
    "    [\"syn_verb_ratio\", 5.5, 0.1892, \"High\"],\n",
    "    [\"lex_flesch_reading_ease\", 6.0, 0.1721, \"High\"],\n",
    "    [\"syn_adj_ratio\", 7.0, -0.2789, \"Low\"],\n",
    "    [\"lex_rare_word_ratio\", 7.0, -0.2774, \"Low\"],\n",
    "    [\"lex_flesch_reading_ease\", 7.0, 0.2558, \"High\"],\n",
    "    [\"lex_flesch_kincaid_grade\", 7.0, -0.2346, \"Low\"],\n",
    "    [\"syn_noun_ratio\", 7.0, -0.1989, \"Low\"],\n",
    "    [\"syn_noun_ratio\", 7.5, -0.32, \"Low\"],\n",
    "    [\"lex_flesch_reading_ease\", 8.0, 0.2809, \"High\"],\n",
    "    [\"syn_adj_ratio\", 8.0, -0.2475, \"Low\"],\n",
    "    [\"lex_flesch_kincaid_grade\", 8.0, -0.2075, \"Low\"],\n",
    "    [\"lex_flesch_reading_ease\", 8.5, 0.2549, \"High\"],\n",
    "    [\"lex_flesch_kincaid_grade\", 8.5, -0.2436, \"Low\"],\n",
    "    [\"lex_flesch_reading_ease\", 9.0, 0.5423, \"High\"]\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Feature\", \"Band\", \"Cohen_d\", \"Disadvantaged_Group\"])\n",
    "\n",
    "# --- Visualization ---\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Create one subplot per band\n",
    "bands = sorted(df[\"Band\"].unique())\n",
    "n_bands = len(bands)\n",
    "\n",
    "fig, axes = plt.subplots(n_bands, 1, figsize=(10, n_bands * 1.8), sharex=True)\n",
    "\n",
    "for i, band in enumerate(bands):\n",
    "    ax = axes[i]\n",
    "    band_df = df[df[\"Band\"] == band].sort_values(\"Cohen_d\")\n",
    "\n",
    "    # Color by disadvantaged group\n",
    "    colors = band_df[\"Disadvantaged_Group\"].map({\"Low\": \"#1f77b4\", \"High\": \"#ff7f0e\"})\n",
    "\n",
    "    ax.barh(band_df[\"Feature\"], band_df[\"Cohen_d\"], color=colors, edgecolor=\"black\")\n",
    "    ax.axvline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "    ax.set_title(f\"Band {band}\", fontsize=12, loc=\"left\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"Cohen's d\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.label_outer()  # show outer labels only (default)\n",
    "    ax.tick_params(labelbottom=True)  # force all to show x-axis ticks\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "# Create legend manually\n",
    "handles = [\n",
    "    plt.Line2D([0], [0], color=\"#1f77b4\", lw=6, label=\"Low (Disadvantaged)\"),\n",
    "    plt.Line2D([0], [0], color=\"#ff7f0e\", lw=6, label=\"High (Disadvantaged)\")\n",
    "]\n",
    "fig.legend(handles=handles, loc=\"upper right\", bbox_to_anchor=(1, 0.97))\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(\"Cohen's d_Effect_Size_by_Band_Feature.png\", dpi=300, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a22b7",
   "metadata": {},
   "source": [
    "### 4.4 SHAP Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc4267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Generating SHAP Visualizations\n",
      "================================================================================\n",
      "✓ Saved: shap_summary_beeswarm.png\n"
     ]
    }
   ],
   "source": [
    "# === Generate SHAP Visualizations ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Generating SHAP Visualizations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # 1. SHAP Summary Beeswarm Plot (shows feature importance + direction)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.summary_plot(\n",
    "        shap_vals, \n",
    "        X_shap,\n",
    "        plot_type=\"dot\",  # Beeswarm plot\n",
    "        max_display=15,   # Show top 15 features\n",
    "        show=False\n",
    "    )\n",
    "    plt.title('SHAP Summary Plot - Top 15 Features', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_summary_beeswarm.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: shap_summary_beeswarm.png\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ SHAP visualization failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
