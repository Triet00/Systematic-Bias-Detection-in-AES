# Systematic-Bias-Detection-in-AES
This project examines the systematic bias in LLMs automated essay scoring in the IELTS writing, using the “\textit{IELTS-writing-task-2-evaluation}” and “\textit{IELTS evaluations}” datasets posted by \cite{chillies2024ielts} via HuggingFace. While the former is used for training the internal models, the latter is used for external validation to test for generalization.

While prior studies mainly examines overall scoring accuracy, this study investigates which specific linguistic features drive subgroup-level bias by examining how linguistic features, such as lexical, syntactic, and discourse-level, contribute to consistent over- or under-scoring of subgroups of linguistic feature tertiles within band scores. The bias magnitude of each feature are computed using the mean absolute discrepancies between human and LLM scores within each subgroup before being ranked accordingly. Predictive models (Random Forest, Logistic Regression, SVM, XGBoost) are then trained without the identified top features to assess whether bias remains predictable. After cross-validation, the best model is tuned and evaluated on an external dataset. The results show that readability metrics are the strongest drivers of systematic bias among the top features; however, when removing these leading metrics, band and the remaining linguistic features still offer a competitive predictive power according to SHAP analysis. Among the models, XGBoost performs best (F1 = 0.9065; ROC-AUC = 0.9247), with high probability accurately identifying bias-prone essays and strong generalizability.

Please note that for all the code snippets to work, you need to have all the csv file in your folder. Enjoy!